{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAY MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Base Model with Attraction Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:50:31.511209Z",
     "start_time": "2018-08-07T01:50:31.284623Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, auc, confusion_matrix, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T13:12:30.342325Z",
     "start_time": "2018-08-08T13:12:29.746543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8368, 305)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>partner</th>\n",
       "      <th>pid</th>\n",
       "      <th>match</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>...</th>\n",
       "      <th>career_c_8.5</th>\n",
       "      <th>career_c_9.0</th>\n",
       "      <th>career_c_10.0</th>\n",
       "      <th>career_c_11.0</th>\n",
       "      <th>career_c_12.0</th>\n",
       "      <th>career_c_13.0</th>\n",
       "      <th>career_c_14.0</th>\n",
       "      <th>career_c_15.0</th>\n",
       "      <th>career_c_16.0</th>\n",
       "      <th>career_c_17.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   iid   id  gender  idg  condtn  partner pid  match  int_corr  samerace  \\\n",
       "0    1  1.0       0    1       1        1  11      0      0.14         0   \n",
       "1    1  1.0       0    1       1        2  12      0      0.54         0   \n",
       "2    1  1.0       0    1       1        3  13      1      0.16         1   \n",
       "3    1  1.0       0    1       1        4  14      1      0.61         0   \n",
       "4    1  1.0       0    1       1        5  15      1      0.21         0   \n",
       "\n",
       "       ...        career_c_8.5  career_c_9.0  career_c_10.0  career_c_11.0  \\\n",
       "0      ...                   0             0              0              0   \n",
       "1      ...                   0             0              0              0   \n",
       "2      ...                   0             0              0              0   \n",
       "3      ...                   0             0              0              0   \n",
       "4      ...                   0             0              0              0   \n",
       "\n",
       "   career_c_12.0  career_c_13.0  career_c_14.0  career_c_15.0  career_c_16.0  \\\n",
       "0              0              0              0              0              0   \n",
       "1              0              0              0              0              0   \n",
       "2              0              0              0              0              0   \n",
       "3              0              0              0              0              0   \n",
       "4              0              0              0              0              0   \n",
       "\n",
       "   career_c_17.0  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "\n",
       "[5 rows x 305 columns]"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('dummied.pkl')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Features and Models\n",
    "I want to have a base model with only attraction features to see how accurate these scores are\n",
    "\n",
    "Note: Precision seeks to answer the question of how many of the selected items were correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### print_confusion_matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T20:30:47.546259Z",
     "start_time": "2018-08-07T20:30:47.478209Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=18):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"PuRd\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T13:12:46.481131Z",
     "start_time": "2018-08-08T13:12:46.460967Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_2 = [c for c in df.columns if '_2' in c]\n",
    "features_2 = features_2[:-17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T13:12:47.121060Z",
     "start_time": "2018-08-08T13:12:47.071097Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [c for c in df.columns if '_3' in c]\n",
    "(features[:37])\n",
    "features = features + features_2\n",
    "drop = features[37:]\n",
    "\n",
    "drop =  drop + ['match', 'career', 'dec_o', 'dec', 'you_call', 'they_cal', 'int_corr', 'iid', 'pid', 'idg', 'id']\n",
    "# drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T13:12:49.018451Z",
     "start_time": "2018-08-08T13:12:48.562167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5857, 250)"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is with all the features without any engineering or selection\n",
    "\n",
    "X = df.drop(drop, axis =1, errors = 'ignore') # i am dropping career bc its not a numeric column :[\n",
    "y = df.match\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 444)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T21:45:34.177934Z",
     "start_time": "2018-08-06T21:45:34.154394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iid',\n",
       " 'id',\n",
       " 'gender',\n",
       " 'idg',\n",
       " 'condtn',\n",
       " 'partner',\n",
       " 'pid',\n",
       " 'int_corr',\n",
       " 'samerace',\n",
       " 'age_o',\n",
       " 'race_o',\n",
       " 'pf_o_att',\n",
       " 'pf_o_sin',\n",
       " 'pf_o_int',\n",
       " 'pf_o_fun',\n",
       " 'pf_o_amb',\n",
       " 'pf_o_sha',\n",
       " 'attr_o',\n",
       " 'sinc_o',\n",
       " 'intel_o',\n",
       " 'fun_o',\n",
       " 'amb_o',\n",
       " 'shar_o',\n",
       " 'like_o',\n",
       " 'prob_o',\n",
       " 'met_o',\n",
       " 'age',\n",
       " 'imprace',\n",
       " 'imprelig',\n",
       " 'income',\n",
       " 'sports',\n",
       " 'tvsports',\n",
       " 'exercise',\n",
       " 'dining',\n",
       " 'museums',\n",
       " 'art',\n",
       " 'hiking',\n",
       " 'gaming',\n",
       " 'clubbing',\n",
       " 'reading',\n",
       " 'tv',\n",
       " 'theater',\n",
       " 'movies',\n",
       " 'concerts',\n",
       " 'music',\n",
       " 'shopping',\n",
       " 'yoga',\n",
       " 'exphappy',\n",
       " 'expnum',\n",
       " 'attr1_1',\n",
       " 'sinc1_1',\n",
       " 'intel1_1',\n",
       " 'fun1_1',\n",
       " 'amb1_1',\n",
       " 'shar1_1',\n",
       " 'attr4_1',\n",
       " 'sinc4_1',\n",
       " 'intel4_1',\n",
       " 'fun4_1',\n",
       " 'amb4_1',\n",
       " 'shar4_1',\n",
       " 'attr2_1',\n",
       " 'sinc2_1',\n",
       " 'intel2_1',\n",
       " 'fun2_1',\n",
       " 'amb2_1',\n",
       " 'shar2_1',\n",
       " 'attr3_1',\n",
       " 'sinc3_1',\n",
       " 'fun3_1',\n",
       " 'intel3_1',\n",
       " 'amb3_1',\n",
       " 'attr5_1',\n",
       " 'sinc5_1',\n",
       " 'intel5_1',\n",
       " 'fun5_1',\n",
       " 'amb5_1',\n",
       " 'attr',\n",
       " 'sinc',\n",
       " 'intel',\n",
       " 'fun',\n",
       " 'amb',\n",
       " 'shar',\n",
       " 'like',\n",
       " 'prob',\n",
       " 'met',\n",
       " 'match_es',\n",
       " 'attr1_s',\n",
       " 'sinc1_s',\n",
       " 'intel1_s',\n",
       " 'fun1_s',\n",
       " 'amb1_s',\n",
       " 'shar1_s',\n",
       " 'attr3_s',\n",
       " 'sinc3_s',\n",
       " 'intel3_s',\n",
       " 'fun3_s',\n",
       " 'amb3_s',\n",
       " 'satis_2',\n",
       " 'length',\n",
       " 'numdat_2',\n",
       " 'attr7_2',\n",
       " 'sinc7_2',\n",
       " 'intel7_2',\n",
       " 'fun7_2',\n",
       " 'amb7_2',\n",
       " 'shar7_2',\n",
       " 'attr1_2',\n",
       " 'sinc1_2',\n",
       " 'intel1_2',\n",
       " 'fun1_2',\n",
       " 'amb1_2',\n",
       " 'shar1_2',\n",
       " 'attr4_2',\n",
       " 'sinc4_2',\n",
       " 'intel4_2',\n",
       " 'fun4_2',\n",
       " 'amb4_2',\n",
       " 'shar4_2',\n",
       " 'attr2_2',\n",
       " 'sinc2_2',\n",
       " 'intel2_2',\n",
       " 'fun2_2',\n",
       " 'amb2_2',\n",
       " 'shar2_2',\n",
       " 'attr3_2',\n",
       " 'sinc3_2',\n",
       " 'intel3_2',\n",
       " 'fun3_2',\n",
       " 'amb3_2',\n",
       " 'attr5_2',\n",
       " 'sinc5_2',\n",
       " 'intel5_2',\n",
       " 'fun5_2',\n",
       " 'amb5_2',\n",
       " 'you_call',\n",
       " 'them_cal',\n",
       " 'date_3',\n",
       " 'numdat_3',\n",
       " 'num_in_3',\n",
       " 'attr1_3',\n",
       " 'sinc1_3',\n",
       " 'intel1_3',\n",
       " 'fun1_3',\n",
       " 'amb1_3',\n",
       " 'shar1_3',\n",
       " 'attr7_3',\n",
       " 'sinc7_3',\n",
       " 'intel7_3',\n",
       " 'fun7_3',\n",
       " 'amb7_3',\n",
       " 'shar7_3',\n",
       " 'attr4_3',\n",
       " 'sinc4_3',\n",
       " 'intel4_3',\n",
       " 'fun4_3',\n",
       " 'amb4_3',\n",
       " 'shar4_3',\n",
       " 'attr2_3',\n",
       " 'sinc2_3',\n",
       " 'intel2_3',\n",
       " 'fun2_3',\n",
       " 'amb2_3',\n",
       " 'shar2_3',\n",
       " 'attr3_3',\n",
       " 'sinc3_3',\n",
       " 'intel3_3',\n",
       " 'fun3_3',\n",
       " 'amb3_3',\n",
       " 'attr5_3',\n",
       " 'sinc5_3',\n",
       " 'intel5_3',\n",
       " 'fun5_3',\n",
       " 'amb5_3',\n",
       " 'wave__1',\n",
       " 'wave__2',\n",
       " 'wave__3',\n",
       " 'wave__4',\n",
       " 'wave__5',\n",
       " 'wave__6',\n",
       " 'wave__7',\n",
       " 'wave__8',\n",
       " 'wave__9',\n",
       " 'wave__10',\n",
       " 'wave__11',\n",
       " 'wave__12',\n",
       " 'wave__13',\n",
       " 'wave__14',\n",
       " 'wave__15',\n",
       " 'wave__16',\n",
       " 'wave__17',\n",
       " 'wave__18',\n",
       " 'wave__19',\n",
       " 'wave__20',\n",
       " 'wave__21',\n",
       " 'positions__1',\n",
       " 'positions__2',\n",
       " 'positions__3',\n",
       " 'positions__4',\n",
       " 'positions__5',\n",
       " 'positions__6',\n",
       " 'positions__7',\n",
       " 'positions__8',\n",
       " 'positions__9',\n",
       " 'positions__10',\n",
       " 'positions__11',\n",
       " 'positions__12',\n",
       " 'positions__13',\n",
       " 'positions__14',\n",
       " 'positions__15',\n",
       " 'positions__16',\n",
       " 'positions__17',\n",
       " 'positions__18',\n",
       " 'positions__19',\n",
       " 'positions__20',\n",
       " 'positions__21',\n",
       " 'positions__22',\n",
       " 'race__1.0',\n",
       " 'race__2.0',\n",
       " 'race__3.0',\n",
       " 'race__4.0',\n",
       " 'race__6.0',\n",
       " 'order__1',\n",
       " 'order__2',\n",
       " 'order__3',\n",
       " 'order__4',\n",
       " 'order__5',\n",
       " 'order__6',\n",
       " 'order__7',\n",
       " 'order__8',\n",
       " 'order__9',\n",
       " 'order__10',\n",
       " 'order__11',\n",
       " 'order__12',\n",
       " 'order__13',\n",
       " 'order__14',\n",
       " 'order__15',\n",
       " 'order__16',\n",
       " 'order__17',\n",
       " 'order__18',\n",
       " 'order__19',\n",
       " 'order__20',\n",
       " 'order__21',\n",
       " 'order__22',\n",
       " 'field_1.0',\n",
       " 'field_2.0',\n",
       " 'field_3.0',\n",
       " 'field_4.0',\n",
       " 'field_5.0',\n",
       " 'field_6.0',\n",
       " 'field_7.0',\n",
       " 'field_8.0',\n",
       " 'field_9.0',\n",
       " 'field_10.0',\n",
       " 'field_11.0',\n",
       " 'field_12.0',\n",
       " 'field_13.0',\n",
       " 'field_14.0',\n",
       " 'field_15.0',\n",
       " 'field_16.0',\n",
       " 'field_17.0',\n",
       " 'field_18.0',\n",
       " 'goal_1.0',\n",
       " 'goal_1.5',\n",
       " 'goal_2.0',\n",
       " 'goal_3.0',\n",
       " 'goal_4.0',\n",
       " 'goal_5.0',\n",
       " 'goal_6.0',\n",
       " 'date_0.0',\n",
       " 'date_1.0',\n",
       " 'date_2.0',\n",
       " 'date_3.0',\n",
       " 'date_4.0',\n",
       " 'date_5.0',\n",
       " 'date_6.0',\n",
       " 'go_out_0.0',\n",
       " 'go_out_1.0',\n",
       " 'go_out_2.0',\n",
       " 'go_out_3.0',\n",
       " 'go_out_4.0',\n",
       " 'go_out_5.0',\n",
       " 'go_out_6.0',\n",
       " 'career_c_1.0',\n",
       " 'career_c_2.0',\n",
       " 'career_c_3.0',\n",
       " 'career_c_4.0',\n",
       " 'career_c_5.0',\n",
       " 'career_c_6.0',\n",
       " 'career_c_7.0',\n",
       " 'career_c_8.0',\n",
       " 'career_c_8.5',\n",
       " 'career_c_9.0',\n",
       " 'career_c_10.0',\n",
       " 'career_c_11.0',\n",
       " 'career_c_12.0',\n",
       " 'career_c_13.0',\n",
       " 'career_c_14.0',\n",
       " 'career_c_15.0',\n",
       " 'career_c_16.0',\n",
       " 'career_c_17.0']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T15:16:47.947037Z",
     "start_time": "2018-08-06T15:16:47.894719Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is with just the attraction features\n",
    "\n",
    "df_base = df[['match', 'attr', 'attr_o']].dropna()\n",
    "\n",
    "X = df_base.drop('match', axis = 1)\n",
    "y = df_base['match']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 444)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on the smaller dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:57:04.478263Z",
     "start_time": "2018-08-07T01:57:04.423991Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: \t0.49782923299565845\n",
      "recall score:  \t 0.494436381229\n",
      "precision score:  \t 0.496357455075\n",
      "f1 score:  \t 0.495395055744\n"
     ]
    }
   ],
   "source": [
    "model_1 = DummyClassifier()\n",
    "model_1.fit(X_train, y_train)\n",
    "y_predict = model_1.predict(X_test)\n",
    "model_1.score(X_test, y_test)\n",
    "\n",
    "print(f'accuracy score: \\t{accuracy_score(y_test, y_predict)}' )\n",
    "print('recall score: ', '\\t', recall_score(y_test, y_predict))\n",
    "print('precision score: ', '\\t', precision_score(y_test, y_predict))\n",
    "print('f1 score: ', '\\t', f1_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T01:58:11.064642Z",
     "start_time": "2018-08-07T01:58:10.233257Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAHXCAYAAAAMbnZJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=y_predict)\n",
    "cm = print_confusion_matrix(conf_mat, ['Class 0', 'Class 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T18:28:37.647001Z",
     "start_time": "2018-08-05T18:28:37.615763Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: \t0.8383114297092792\n",
      "recall score:  \t 0.099537037037\n",
      "precision score:  \t 0.716666666667\n",
      "f1 score:  \t 0.174796747967\n"
     ]
    }
   ],
   "source": [
    "model_2 = LogisticRegression()\n",
    "model_2.fit(X_train, y_train)\n",
    "y_predict = model_2.predict(X_test)\n",
    "model_2.score(X_test, y_test)\n",
    "\n",
    "print(f'accuracy score: \\t{accuracy_score(y_test, y_predict)}' )\n",
    "print('recall score: ', '\\t', recall_score(y_test, y_predict))\n",
    "print('precision score: ', '\\t', precision_score(y_test, y_predict))\n",
    "print('f1 score: ', '\\t', f1_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T18:28:56.424091Z",
     "start_time": "2018-08-05T18:28:56.213245Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAHXCAYAAAAMbnZJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=y_predict)\n",
    "cm = print_confusion_matrix(conf_mat, ['Class 0', 'Class 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Decision Tree time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T15:03:00.255452Z",
     "start_time": "2018-08-06T15:03:00.067868Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Business - Investment Management'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-54f47f8cf0bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'accuracy score: \\t{accuracy_score(y_test, y_predict)}'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Business - Investment Management'"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "print(f'accuracy score: \\t{accuracy_score(y_test, y_predict)}' )\n",
    "print('recall score: ', '\\t', recall_score(y_test, y_predict))\n",
    "print('precision score: ', '\\t', precision_score(y_test, y_predict))\n",
    "print('f1 score: ', '\\t', f1_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T18:29:02.824840Z",
     "start_time": "2018-08-05T18:29:02.641100Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAHZCAYAAAA2ZBc5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFfX/g+a6lL6lVZwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=y_predict)\n",
    "cm = print_confusion_matrix(conf_mat, ['Class 0', 'Class 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Features I have available for the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T15:19:36.098805Z",
     "start_time": "2018-08-06T15:19:36.076679Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iid',\n",
       " 'id',\n",
       " 'gender',\n",
       " 'idg',\n",
       " 'condtn',\n",
       " 'partner',\n",
       " 'pid',\n",
       " 'int_corr',\n",
       " 'samerace',\n",
       " 'age_o',\n",
       " 'race_o',\n",
       " 'pf_o_att',\n",
       " 'pf_o_sin',\n",
       " 'pf_o_int',\n",
       " 'pf_o_fun',\n",
       " 'pf_o_amb',\n",
       " 'pf_o_sha',\n",
       " 'dec_o',\n",
       " 'attr_o',\n",
       " 'sinc_o',\n",
       " 'intel_o',\n",
       " 'fun_o',\n",
       " 'amb_o',\n",
       " 'shar_o',\n",
       " 'like_o',\n",
       " 'prob_o',\n",
       " 'met_o',\n",
       " 'age',\n",
       " 'imprace',\n",
       " 'imprelig',\n",
       " 'income',\n",
       " 'sports',\n",
       " 'tvsports',\n",
       " 'exercise',\n",
       " 'dining',\n",
       " 'museums',\n",
       " 'art',\n",
       " 'hiking',\n",
       " 'gaming',\n",
       " 'clubbing',\n",
       " 'reading',\n",
       " 'tv',\n",
       " 'theater',\n",
       " 'movies',\n",
       " 'concerts',\n",
       " 'music',\n",
       " 'shopping',\n",
       " 'yoga',\n",
       " 'exphappy',\n",
       " 'expnum',\n",
       " 'attr1_1',\n",
       " 'sinc1_1',\n",
       " 'intel1_1',\n",
       " 'fun1_1',\n",
       " 'amb1_1',\n",
       " 'shar1_1',\n",
       " 'attr4_1',\n",
       " 'sinc4_1',\n",
       " 'intel4_1',\n",
       " 'fun4_1',\n",
       " 'amb4_1',\n",
       " 'shar4_1',\n",
       " 'attr2_1',\n",
       " 'sinc2_1',\n",
       " 'intel2_1',\n",
       " 'fun2_1',\n",
       " 'amb2_1',\n",
       " 'shar2_1',\n",
       " 'attr3_1',\n",
       " 'sinc3_1',\n",
       " 'fun3_1',\n",
       " 'intel3_1',\n",
       " 'amb3_1',\n",
       " 'attr5_1',\n",
       " 'sinc5_1',\n",
       " 'intel5_1',\n",
       " 'fun5_1',\n",
       " 'amb5_1',\n",
       " 'dec',\n",
       " 'attr',\n",
       " 'sinc',\n",
       " 'intel',\n",
       " 'fun',\n",
       " 'amb',\n",
       " 'shar',\n",
       " 'like',\n",
       " 'prob',\n",
       " 'met',\n",
       " 'match_es',\n",
       " 'attr1_s',\n",
       " 'sinc1_s',\n",
       " 'intel1_s',\n",
       " 'fun1_s',\n",
       " 'amb1_s',\n",
       " 'shar1_s',\n",
       " 'attr3_s',\n",
       " 'sinc3_s',\n",
       " 'intel3_s',\n",
       " 'fun3_s',\n",
       " 'amb3_s',\n",
       " 'satis_2',\n",
       " 'length',\n",
       " 'numdat_2',\n",
       " 'attr7_2',\n",
       " 'sinc7_2',\n",
       " 'intel7_2',\n",
       " 'fun7_2',\n",
       " 'amb7_2',\n",
       " 'shar7_2',\n",
       " 'attr1_2',\n",
       " 'sinc1_2',\n",
       " 'intel1_2',\n",
       " 'fun1_2',\n",
       " 'amb1_2',\n",
       " 'shar1_2',\n",
       " 'attr4_2',\n",
       " 'sinc4_2',\n",
       " 'intel4_2',\n",
       " 'fun4_2',\n",
       " 'amb4_2',\n",
       " 'shar4_2',\n",
       " 'attr2_2',\n",
       " 'sinc2_2',\n",
       " 'intel2_2',\n",
       " 'fun2_2',\n",
       " 'amb2_2',\n",
       " 'shar2_2',\n",
       " 'attr3_2',\n",
       " 'sinc3_2',\n",
       " 'intel3_2',\n",
       " 'fun3_2',\n",
       " 'amb3_2',\n",
       " 'attr5_2',\n",
       " 'sinc5_2',\n",
       " 'intel5_2',\n",
       " 'fun5_2',\n",
       " 'amb5_2',\n",
       " 'you_call',\n",
       " 'them_cal',\n",
       " 'date_3',\n",
       " 'numdat_3',\n",
       " 'num_in_3',\n",
       " 'attr1_3',\n",
       " 'sinc1_3',\n",
       " 'intel1_3',\n",
       " 'fun1_3',\n",
       " 'amb1_3',\n",
       " 'shar1_3',\n",
       " 'attr7_3',\n",
       " 'sinc7_3',\n",
       " 'intel7_3',\n",
       " 'fun7_3',\n",
       " 'amb7_3',\n",
       " 'shar7_3',\n",
       " 'attr4_3',\n",
       " 'sinc4_3',\n",
       " 'intel4_3',\n",
       " 'fun4_3',\n",
       " 'amb4_3',\n",
       " 'shar4_3',\n",
       " 'attr2_3',\n",
       " 'sinc2_3',\n",
       " 'intel2_3',\n",
       " 'fun2_3',\n",
       " 'amb2_3',\n",
       " 'shar2_3',\n",
       " 'attr3_3',\n",
       " 'sinc3_3',\n",
       " 'intel3_3',\n",
       " 'fun3_3',\n",
       " 'amb3_3',\n",
       " 'attr5_3',\n",
       " 'sinc5_3',\n",
       " 'intel5_3',\n",
       " 'fun5_3',\n",
       " 'amb5_3',\n",
       " 'wave__1',\n",
       " 'wave__2',\n",
       " 'wave__3',\n",
       " 'wave__4',\n",
       " 'wave__5',\n",
       " 'wave__6',\n",
       " 'wave__7',\n",
       " 'wave__8',\n",
       " 'wave__9',\n",
       " 'wave__10',\n",
       " 'wave__11',\n",
       " 'wave__12',\n",
       " 'wave__13',\n",
       " 'wave__14',\n",
       " 'wave__15',\n",
       " 'wave__16',\n",
       " 'wave__17',\n",
       " 'wave__18',\n",
       " 'wave__19',\n",
       " 'wave__20',\n",
       " 'wave__21',\n",
       " 'positions__1',\n",
       " 'positions__2',\n",
       " 'positions__3',\n",
       " 'positions__4',\n",
       " 'positions__5',\n",
       " 'positions__6',\n",
       " 'positions__7',\n",
       " 'positions__8',\n",
       " 'positions__9',\n",
       " 'positions__10',\n",
       " 'positions__11',\n",
       " 'positions__12',\n",
       " 'positions__13',\n",
       " 'positions__14',\n",
       " 'positions__15',\n",
       " 'positions__16',\n",
       " 'positions__17',\n",
       " 'positions__18',\n",
       " 'positions__19',\n",
       " 'positions__20',\n",
       " 'positions__21',\n",
       " 'positions__22',\n",
       " 'race__1.0',\n",
       " 'race__2.0',\n",
       " 'race__3.0',\n",
       " 'race__4.0',\n",
       " 'race__6.0',\n",
       " 'order__1',\n",
       " 'order__2',\n",
       " 'order__3',\n",
       " 'order__4',\n",
       " 'order__5',\n",
       " 'order__6',\n",
       " 'order__7',\n",
       " 'order__8',\n",
       " 'order__9',\n",
       " 'order__10',\n",
       " 'order__11',\n",
       " 'order__12',\n",
       " 'order__13',\n",
       " 'order__14',\n",
       " 'order__15',\n",
       " 'order__16',\n",
       " 'order__17',\n",
       " 'order__18',\n",
       " 'order__19',\n",
       " 'order__20',\n",
       " 'order__21',\n",
       " 'order__22',\n",
       " 'field_1.0',\n",
       " 'field_2.0',\n",
       " 'field_3.0',\n",
       " 'field_4.0',\n",
       " 'field_5.0',\n",
       " 'field_6.0',\n",
       " 'field_7.0',\n",
       " 'field_8.0',\n",
       " 'field_9.0',\n",
       " 'field_10.0',\n",
       " 'field_11.0',\n",
       " 'field_12.0',\n",
       " 'field_13.0',\n",
       " 'field_14.0',\n",
       " 'field_15.0',\n",
       " 'field_16.0',\n",
       " 'field_17.0',\n",
       " 'field_18.0',\n",
       " 'goal_1.0',\n",
       " 'goal_1.5',\n",
       " 'goal_2.0',\n",
       " 'goal_3.0',\n",
       " 'goal_4.0',\n",
       " 'goal_5.0',\n",
       " 'goal_6.0',\n",
       " 'date_0.0',\n",
       " 'date_1.0',\n",
       " 'date_2.0',\n",
       " 'date_3.0',\n",
       " 'date_4.0',\n",
       " 'date_5.0',\n",
       " 'date_6.0',\n",
       " 'go_out_0.0',\n",
       " 'go_out_1.0',\n",
       " 'go_out_2.0',\n",
       " 'go_out_3.0',\n",
       " 'go_out_4.0',\n",
       " 'go_out_5.0',\n",
       " 'go_out_6.0',\n",
       " 'career_c_1.0',\n",
       " 'career_c_2.0',\n",
       " 'career_c_3.0',\n",
       " 'career_c_4.0',\n",
       " 'career_c_5.0',\n",
       " 'career_c_6.0',\n",
       " 'career_c_7.0',\n",
       " 'career_c_8.0',\n",
       " 'career_c_8.5',\n",
       " 'career_c_9.0',\n",
       " 'career_c_10.0',\n",
       " 'career_c_11.0',\n",
       " 'career_c_12.0',\n",
       " 'career_c_13.0',\n",
       " 'career_c_14.0',\n",
       " 'career_c_15.0',\n",
       " 'career_c_16.0',\n",
       " 'career_c_17.0']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T20:50:21.602699Z",
     "start_time": "2018-08-06T20:50:21.261437Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ssX = StandardScaler()\n",
    "X_train_scaled = ssX.fit_transform(X_train)\n",
    "X_test_scaled = ssX.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T19:36:24.849870Z",
     "start_time": "2018-08-06T19:16:17.118201Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "knn: best score: 0.8615161857482676\n",
      "recall score:  \t 0.984096385542\n",
      "\n",
      "logistic: best score: 1.0\n",
      "recall score:  \t 1.0\n",
      "\n",
      "decision tree: best score: 1.0\n",
      "recall score:  \t 1.0\n",
      "\n",
      "random forest: best score: 0.9665942703485365\n",
      "recall score:  \t 0.961927710843\n"
     ]
    }
   ],
   "source": [
    "models = [('knn', KNN), \n",
    "          ('logistic', LogisticRegression),\n",
    "          ('decision tree', DecisionTreeClassifier),\n",
    "          ('random forest', RandomForestClassifier)\n",
    "         ]\n",
    "\n",
    "param_choices = [\n",
    "    {\n",
    "        'n_neighbors': range(1, 12)\n",
    "    },\n",
    "    {\n",
    "        'C': np.logspace(-3,6, 12),\n",
    "        'penalty': ['l1', 'l2']\n",
    "    },\n",
    "    {\n",
    "        'max_depth': [1,2,3,4,5],\n",
    "        'min_samples_leaf': [3,6,10]\n",
    "    },\n",
    "    {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [1,2,3,4,5],\n",
    "        'min_samples_leaf': [3,6,10]\n",
    "    }\n",
    "]\n",
    "\n",
    "grids = {}\n",
    "for model_info, params in zip(models, param_choices):\n",
    "    name, model = model_info\n",
    "    grid = GridSearchCV(model(), params)\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    y_predict = grid.predict(X_test_scaled)\n",
    "    s = f\"\\n{name}: best score: {grid.best_score_}\"\n",
    "    print(s)\n",
    "    # cvs = np.mean(cross_val_score(grid, X_train_scaled, y_train, cv = 10, scoring= 'recall'))\n",
    "    # print(f\"this is the best indicator of how well the model is working {cvs}\")\n",
    "    print('recall score: ', '\\t', recall_score(y_test, y_predict))\n",
    "    # print(f'f1 score: {f1_score(y_test, y_predict)}')\n",
    "    # I also want to get the recall score because that is the metric that i care about\n",
    "    grids[name] = grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T20:45:05.290141Z",
     "start_time": "2018-08-06T20:45:00.233694Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000\n",
      "Recall: 1.000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      2069\n",
      "          1       1.00      1.00      1.00      2075\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4144\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=3, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wow decision tree looks great\n",
    "params = {\n",
    "        'max_depth': [1,2,3,4,5],\n",
    "        'min_samples_leaf': [3,6,10]\n",
    "    }\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), params)\n",
    "\n",
    "\n",
    "#model = DecisionTreeClassifier(max_depth= [1,2,3,4,5], min_samples_leaf= [3,6,10])\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "y_predict = grid.predict(X_test_scaled)\n",
    "\n",
    "print(\"Accuracy: %.3f\"% accuracy_score(y_test, grid.predict(X_test_scaled)))\n",
    "print(\"Recall: %.3f\"% recall_score(y_test, grid.predict(X_test_scaled)))\n",
    "print(classification_report(y_test, grid.predict(X_test_scaled)))\n",
    "\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T20:47:18.663451Z",
     "start_time": "2018-08-06T20:47:18.318204Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=3, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best')\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "model.score(X_test_scaled, y_test)\n",
    "feature_viewer = {}\n",
    "for col, score in zip(X.columns, model.feature_importances_):\n",
    "    feature_viewer[col] = score\n",
    "\n",
    "    \n",
    "#df = pd.DataFrame.from_dict(feature_viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T20:47:28.111102Z",
     "start_time": "2018-08-06T20:47:28.091093Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 0.0,\n",
       " 'age_o': 0.0,\n",
       " 'amb': 0.0,\n",
       " 'amb1_1': 0.0,\n",
       " 'amb1_2': 0.0,\n",
       " 'amb1_3': 0.0,\n",
       " 'amb1_s': 0.0,\n",
       " 'amb2_1': 0.0,\n",
       " 'amb2_2': 0.0,\n",
       " 'amb2_3': 0.0,\n",
       " 'amb3_1': 0.0,\n",
       " 'amb3_2': 0.0,\n",
       " 'amb3_3': 0.0,\n",
       " 'amb3_s': 0.0,\n",
       " 'amb4_1': 0.0,\n",
       " 'amb4_2': 0.0,\n",
       " 'amb4_3': 0.0,\n",
       " 'amb5_1': 0.0,\n",
       " 'amb5_2': 0.0,\n",
       " 'amb5_3': 0.0,\n",
       " 'amb7_2': 0.0,\n",
       " 'amb7_3': 0.0,\n",
       " 'amb_o': 0.0,\n",
       " 'art': 0.0,\n",
       " 'attr': 0.0,\n",
       " 'attr1_1': 0.0,\n",
       " 'attr1_2': 0.0,\n",
       " 'attr1_3': 0.0,\n",
       " 'attr1_s': 0.0,\n",
       " 'attr2_1': 0.0,\n",
       " 'attr2_2': 0.0,\n",
       " 'attr2_3': 0.0,\n",
       " 'attr3_1': 0.0,\n",
       " 'attr3_2': 0.0,\n",
       " 'attr3_3': 0.0,\n",
       " 'attr3_s': 0.0,\n",
       " 'attr4_1': 0.0,\n",
       " 'attr4_2': 0.0,\n",
       " 'attr4_3': 0.0,\n",
       " 'attr5_1': 0.0,\n",
       " 'attr5_2': 0.0,\n",
       " 'attr5_3': 0.0,\n",
       " 'attr7_2': 0.0,\n",
       " 'attr7_3': 0.0,\n",
       " 'attr_o': 0.0,\n",
       " 'career_c_1.0': 0.0,\n",
       " 'career_c_10.0': 0.0,\n",
       " 'career_c_11.0': 0.0,\n",
       " 'career_c_12.0': 0.0,\n",
       " 'career_c_13.0': 0.0,\n",
       " 'career_c_14.0': 0.0,\n",
       " 'career_c_15.0': 0.0,\n",
       " 'career_c_16.0': 0.0,\n",
       " 'career_c_17.0': 0.0,\n",
       " 'career_c_2.0': 0.0,\n",
       " 'career_c_3.0': 0.0,\n",
       " 'career_c_4.0': 0.0,\n",
       " 'career_c_5.0': 0.0,\n",
       " 'career_c_6.0': 0.0,\n",
       " 'career_c_7.0': 0.0,\n",
       " 'career_c_8.0': 0.0,\n",
       " 'career_c_8.5': 0.0,\n",
       " 'career_c_9.0': 0.0,\n",
       " 'clubbing': 0.0,\n",
       " 'concerts': 0.0,\n",
       " 'condtn': 0.0,\n",
       " 'date_0.0': 0.0,\n",
       " 'date_1.0': 0.0,\n",
       " 'date_2.0': 0.0,\n",
       " 'date_3': 0.0,\n",
       " 'date_3.0': 0.0,\n",
       " 'date_4.0': 0.0,\n",
       " 'date_5.0': 0.0,\n",
       " 'date_6.0': 0.0,\n",
       " 'dec': 0.4717544216304127,\n",
       " 'dec_o': 0.52824557836958741,\n",
       " 'dining': 0.0,\n",
       " 'exercise': 0.0,\n",
       " 'exphappy': 0.0,\n",
       " 'expnum': 0.0,\n",
       " 'field_1.0': 0.0,\n",
       " 'field_10.0': 0.0,\n",
       " 'field_11.0': 0.0,\n",
       " 'field_12.0': 0.0,\n",
       " 'field_13.0': 0.0,\n",
       " 'field_14.0': 0.0,\n",
       " 'field_15.0': 0.0,\n",
       " 'field_16.0': 0.0,\n",
       " 'field_17.0': 0.0,\n",
       " 'field_18.0': 0.0,\n",
       " 'field_2.0': 0.0,\n",
       " 'field_3.0': 0.0,\n",
       " 'field_4.0': 0.0,\n",
       " 'field_5.0': 0.0,\n",
       " 'field_6.0': 0.0,\n",
       " 'field_7.0': 0.0,\n",
       " 'field_8.0': 0.0,\n",
       " 'field_9.0': 0.0,\n",
       " 'fun': 0.0,\n",
       " 'fun1_1': 0.0,\n",
       " 'fun1_2': 0.0,\n",
       " 'fun1_3': 0.0,\n",
       " 'fun1_s': 0.0,\n",
       " 'fun2_1': 0.0,\n",
       " 'fun2_2': 0.0,\n",
       " 'fun2_3': 0.0,\n",
       " 'fun3_1': 0.0,\n",
       " 'fun3_2': 0.0,\n",
       " 'fun3_3': 0.0,\n",
       " 'fun3_s': 0.0,\n",
       " 'fun4_1': 0.0,\n",
       " 'fun4_2': 0.0,\n",
       " 'fun4_3': 0.0,\n",
       " 'fun5_1': 0.0,\n",
       " 'fun5_2': 0.0,\n",
       " 'fun5_3': 0.0,\n",
       " 'fun7_2': 0.0,\n",
       " 'fun7_3': 0.0,\n",
       " 'fun_o': 0.0,\n",
       " 'gaming': 0.0,\n",
       " 'gender': 0.0,\n",
       " 'go_out_0.0': 0.0,\n",
       " 'go_out_1.0': 0.0,\n",
       " 'go_out_2.0': 0.0,\n",
       " 'go_out_3.0': 0.0,\n",
       " 'go_out_4.0': 0.0,\n",
       " 'go_out_5.0': 0.0,\n",
       " 'go_out_6.0': 0.0,\n",
       " 'goal_1.0': 0.0,\n",
       " 'goal_1.5': 0.0,\n",
       " 'goal_2.0': 0.0,\n",
       " 'goal_3.0': 0.0,\n",
       " 'goal_4.0': 0.0,\n",
       " 'goal_5.0': 0.0,\n",
       " 'goal_6.0': 0.0,\n",
       " 'hiking': 0.0,\n",
       " 'id': 0.0,\n",
       " 'idg': 0.0,\n",
       " 'iid': 0.0,\n",
       " 'imprace': 0.0,\n",
       " 'imprelig': 0.0,\n",
       " 'income': 0.0,\n",
       " 'int_corr': 0.0,\n",
       " 'intel': 0.0,\n",
       " 'intel1_1': 0.0,\n",
       " 'intel1_2': 0.0,\n",
       " 'intel1_3': 0.0,\n",
       " 'intel1_s': 0.0,\n",
       " 'intel2_1': 0.0,\n",
       " 'intel2_2': 0.0,\n",
       " 'intel2_3': 0.0,\n",
       " 'intel3_1': 0.0,\n",
       " 'intel3_2': 0.0,\n",
       " 'intel3_3': 0.0,\n",
       " 'intel3_s': 0.0,\n",
       " 'intel4_1': 0.0,\n",
       " 'intel4_2': 0.0,\n",
       " 'intel4_3': 0.0,\n",
       " 'intel5_1': 0.0,\n",
       " 'intel5_2': 0.0,\n",
       " 'intel5_3': 0.0,\n",
       " 'intel7_2': 0.0,\n",
       " 'intel7_3': 0.0,\n",
       " 'intel_o': 0.0,\n",
       " 'length': 0.0,\n",
       " 'like': 0.0,\n",
       " 'like_o': 0.0,\n",
       " 'match_es': 0.0,\n",
       " 'met': 0.0,\n",
       " 'met_o': 0.0,\n",
       " 'movies': 0.0,\n",
       " 'museums': 0.0,\n",
       " 'music': 0.0,\n",
       " 'num_in_3': 0.0,\n",
       " 'numdat_2': 0.0,\n",
       " 'numdat_3': 0.0,\n",
       " 'order__1': 0.0,\n",
       " 'order__10': 0.0,\n",
       " 'order__11': 0.0,\n",
       " 'order__12': 0.0,\n",
       " 'order__13': 0.0,\n",
       " 'order__14': 0.0,\n",
       " 'order__15': 0.0,\n",
       " 'order__16': 0.0,\n",
       " 'order__17': 0.0,\n",
       " 'order__18': 0.0,\n",
       " 'order__19': 0.0,\n",
       " 'order__2': 0.0,\n",
       " 'order__20': 0.0,\n",
       " 'order__21': 0.0,\n",
       " 'order__22': 0.0,\n",
       " 'order__3': 0.0,\n",
       " 'order__4': 0.0,\n",
       " 'order__5': 0.0,\n",
       " 'order__6': 0.0,\n",
       " 'order__7': 0.0,\n",
       " 'order__8': 0.0,\n",
       " 'order__9': 0.0,\n",
       " 'partner': 0.0,\n",
       " 'pf_o_amb': 0.0,\n",
       " 'pf_o_att': 0.0,\n",
       " 'pf_o_fun': 0.0,\n",
       " 'pf_o_int': 0.0,\n",
       " 'pf_o_sha': 0.0,\n",
       " 'pf_o_sin': 0.0,\n",
       " 'pid': 0.0,\n",
       " 'positions__1': 0.0,\n",
       " 'positions__10': 0.0,\n",
       " 'positions__11': 0.0,\n",
       " 'positions__12': 0.0,\n",
       " 'positions__13': 0.0,\n",
       " 'positions__14': 0.0,\n",
       " 'positions__15': 0.0,\n",
       " 'positions__16': 0.0,\n",
       " 'positions__17': 0.0,\n",
       " 'positions__18': 0.0,\n",
       " 'positions__19': 0.0,\n",
       " 'positions__2': 0.0,\n",
       " 'positions__20': 0.0,\n",
       " 'positions__21': 0.0,\n",
       " 'positions__22': 0.0,\n",
       " 'positions__3': 0.0,\n",
       " 'positions__4': 0.0,\n",
       " 'positions__5': 0.0,\n",
       " 'positions__6': 0.0,\n",
       " 'positions__7': 0.0,\n",
       " 'positions__8': 0.0,\n",
       " 'positions__9': 0.0,\n",
       " 'prob': 0.0,\n",
       " 'prob_o': 0.0,\n",
       " 'race__1.0': 0.0,\n",
       " 'race__2.0': 0.0,\n",
       " 'race__3.0': 0.0,\n",
       " 'race__4.0': 0.0,\n",
       " 'race__6.0': 0.0,\n",
       " 'race_o': 0.0,\n",
       " 'reading': 0.0,\n",
       " 'samerace': 0.0,\n",
       " 'satis_2': 0.0,\n",
       " 'shar': 0.0,\n",
       " 'shar1_1': 0.0,\n",
       " 'shar1_2': 0.0,\n",
       " 'shar1_3': 0.0,\n",
       " 'shar1_s': 0.0,\n",
       " 'shar2_1': 0.0,\n",
       " 'shar2_2': 0.0,\n",
       " 'shar2_3': 0.0,\n",
       " 'shar4_1': 0.0,\n",
       " 'shar4_2': 0.0,\n",
       " 'shar4_3': 0.0,\n",
       " 'shar7_2': 0.0,\n",
       " 'shar7_3': 0.0,\n",
       " 'shar_o': 0.0,\n",
       " 'shopping': 0.0,\n",
       " 'sinc': 0.0,\n",
       " 'sinc1_1': 0.0,\n",
       " 'sinc1_2': 0.0,\n",
       " 'sinc1_3': 0.0,\n",
       " 'sinc1_s': 0.0,\n",
       " 'sinc2_1': 0.0,\n",
       " 'sinc2_2': 0.0,\n",
       " 'sinc2_3': 0.0,\n",
       " 'sinc3_1': 0.0,\n",
       " 'sinc3_2': 0.0,\n",
       " 'sinc3_3': 0.0,\n",
       " 'sinc3_s': 0.0,\n",
       " 'sinc4_1': 0.0,\n",
       " 'sinc4_2': 0.0,\n",
       " 'sinc4_3': 0.0,\n",
       " 'sinc5_1': 0.0,\n",
       " 'sinc5_2': 0.0,\n",
       " 'sinc5_3': 0.0,\n",
       " 'sinc7_2': 0.0,\n",
       " 'sinc7_3': 0.0,\n",
       " 'sinc_o': 0.0,\n",
       " 'sports': 0.0,\n",
       " 'theater': 0.0,\n",
       " 'them_cal': 0.0,\n",
       " 'tv': 0.0,\n",
       " 'tvsports': 0.0,\n",
       " 'wave__1': 0.0,\n",
       " 'wave__10': 0.0,\n",
       " 'wave__11': 0.0,\n",
       " 'wave__12': 0.0,\n",
       " 'wave__13': 0.0,\n",
       " 'wave__14': 0.0,\n",
       " 'wave__15': 0.0,\n",
       " 'wave__16': 0.0,\n",
       " 'wave__17': 0.0,\n",
       " 'wave__18': 0.0,\n",
       " 'wave__19': 0.0,\n",
       " 'wave__2': 0.0,\n",
       " 'wave__20': 0.0,\n",
       " 'wave__21': 0.0,\n",
       " 'wave__3': 0.0,\n",
       " 'wave__4': 0.0,\n",
       " 'wave__5': 0.0,\n",
       " 'wave__6': 0.0,\n",
       " 'wave__7': 0.0,\n",
       " 'wave__8': 0.0,\n",
       " 'wave__9': 0.0,\n",
       " 'yoga': 0.0,\n",
       " 'you_call': 0.0}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T19:53:23.756005Z",
     "start_time": "2018-08-06T19:53:22.183902Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-3075af5319a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_viewer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, data, orient, dtype)\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'only recognize index or columns for orient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    273\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    274\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   5494\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5495\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5496\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5497\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5498\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   5533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5535\u001b[0;31m             raise ValueError('If using all scalar values, you must pass'\n\u001b[0m\u001b[1;32m   5536\u001b[0m                              ' an index')\n\u001b[1;32m   5537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth=10)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "model.score(X_test_scaled, y_test)\n",
    "feature_viewer = {}\n",
    "for col, score in zip(X.columns, model.feature_importances_):\n",
    "    feature_viewer[col] = score\n",
    "\n",
    "    \n",
    "df = pd.DataFrame.from_dict(feature_viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T19:57:48.195159Z",
     "start_time": "2018-08-06T19:57:48.183397Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 0], dtype='object')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(feature_viewer, orient = 'index')\n",
    "df = df.reset_index()\n",
    "df.rename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for key, value in sorted(mydict.iteritems(), key=lambda (k,v): (v,k)):\n",
    "    print \"%s: %s\" % (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T19:43:58.539418Z",
     "start_time": "2018-08-06T19:43:58.354597Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T16:03:51.895626Z",
     "start_time": "2018-08-06T16:03:51.855889Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T16:03:55.621607Z",
     "start_time": "2018-08-06T16:03:54.817543Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_sample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T22:06:32.375493Z",
     "start_time": "2018-08-06T22:06:31.874005Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ssX = StandardScaler()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 444)\n",
    "\n",
    "\n",
    "X_train_scaled = ssX.fit_transform(X_train)\n",
    "X_test_scaled = ssX.fit(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T13:59:44.601399Z",
     "start_time": "2018-08-06T13:59:40.594137Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "score() missing 2 required positional arguments: 'X' and 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d26fcd6f97eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf_ros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf_ros\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: score() missing 2 required positional arguments: 'X' and 'y'"
     ]
    }
   ],
   "source": [
    "clf_ros = SVC().fit(X_resampled, y_resampled)\n",
    "# clf_ros.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T16:09:39.020671Z",
     "start_time": "2018-08-06T16:09:24.692375Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# X_smoted, y_smoted = SMOTE(random_state=42).fit_sample(X,y)\n",
    "\n",
    "# ssX = StandardScaler()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_smoted, y_smoted, test_size = 0.3, random_state = 444)\n",
    "\n",
    "\n",
    "# X_train_scaled = ssX.fit_transform(X_train)\n",
    "# X_test_scaled = ssX.fit(X_test)\n",
    "# clf_smote = SVC().fit(X_train_scaled, y_train)\n",
    "# # X_test_scaled = pd.DataFrame(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T16:10:00.150669Z",
     "start_time": "2018-08-06T16:09:59.905434Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'StandardScaler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-24d227965ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_smote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Class 0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Class 1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseSVC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \"\"\"\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'support_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'StandardScaler'"
     ]
    }
   ],
   "source": [
    "# cm = print_confusion_matrix(confusion_matrix(y, clf_smote.predict(X_test_scaled)), ['Class 0', 'Class 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T17:28:23.250801Z",
     "start_time": "2018-08-06T17:28:07.860685Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ssX = StandardScaler()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_adasyn, y_adasyn, test_size = 0.3, random_state = 444)\n",
    "\n",
    "\n",
    "# X_train_scaled = ssX.fit_transform(X_train)\n",
    "# X_test_scaled = ssX.fit(X_test)\n",
    "#clf_smote = SVC().fit(X_train_scaled, y_train)\n",
    "# X_test_scaled = pd.DataFrame(X_test_scaled)\n",
    "\n",
    "# clf_adasyn = SVC().fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you have to be smart and oversample to make this balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T13:13:08.174164Z",
     "start_time": "2018-08-08T13:13:05.273647Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "X_adasyn, y_adasyn = ADASYN(random_state=42).fit_sample(X,y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_adasyn, y_adasyn, test_size = 0.3, random_state = 444)\n",
    "\n",
    "\n",
    "ssX = StandardScaler()\n",
    "X_train_scaled = ssX.fit_transform(X_train)\n",
    "X_test_scaled = ssX.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T20:22:13.787354Z",
     "start_time": "2018-08-07T20:22:13.779547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.02304011, -0.49040909, -1.55905046, ..., -0.0947187 ,\n",
       "        -0.07548381, -0.03366821],\n",
       "       [ 1.02304011, -0.49040909, -0.76331313, ..., -0.0947187 ,\n",
       "        -0.07548381, -0.03366821],\n",
       "       [-1.02387148, -0.49040909, -1.16777186, ..., -0.0947187 ,\n",
       "        -0.07548381, -0.03366821],\n",
       "       ..., \n",
       "       [ 1.02304011,  2.23515631, -1.55905046, ..., -0.0947187 ,\n",
       "        -0.07548381, -0.03366821],\n",
       "       [-1.02387148, -0.49040909,  0.08485298, ..., -0.0947187 ,\n",
       "        -0.07548381, -0.03366821],\n",
       "       [ 1.02304011, -0.49040909,  1.57117838, ..., -0.0947187 ,\n",
       "        -0.07548381, -0.03366821]])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T22:07:25.695454Z",
     "start_time": "2018-08-06T22:07:00.142402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall score: 0.9545234639574263\n",
      "f1 score: 0.9252051582649473\n"
     ]
    }
   ],
   "source": [
    "# clf_adasyn = SVC().fit(X_train_scaled, y_train)\n",
    "\n",
    "# y_predict = clf_adasyn.predict(X_test_scaled)\n",
    "# print(f'recall score: {recall_score(y_test, y_predict)}')\n",
    "\n",
    "# print(f'f1 score: {f1_score(y_test, y_predict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T22:13:26.396123Z",
     "start_time": "2018-08-07T22:11:43.678562Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-508-39b0800d2f16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mroc_plotting_stuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# drawing graphs\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "models = [RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth=10), \n",
    "          DecisionTreeClassifier(max_depth=None), \n",
    "          # BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100, n_jobs=-1),\n",
    "          SVC(probability=True), \n",
    "          LogisticRegression(C=81.113083078968728, penalty = 'l1'), \n",
    "          SGDClassifier(loss='log', random_state=42, alpha=0.01)]\n",
    "model_names = [\"Random Forest\", \"Decision Tree\",'SVC','Logistic','SGD-Log']\n",
    "\n",
    "roc_plotting_stuff = []\n",
    "for clf, name in zip(models, model_names):\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    preds = clf.predict_proba(X_test_scaled)\n",
    "    fpr, tpr, _ = roc_curve(y_test, preds[:,1])\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    roc_plotting_stuff.append((name, tpr, fpr, auc_score))\n",
    "    \n",
    "    \n",
    "plt.figure(dpi=150)\n",
    "for name, tpr, fpr, auc_score in roc_plotting_stuff:\n",
    "    plt.plot(fpr, tpr, label=name+' (auc: %.2f)'%auc_score)\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.plot([0, 1], [0, 1], color='k', linestyle='--');\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Comparing ROC Curves\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T13:21:01.871077Z",
     "start_time": "2018-08-08T13:20:59.158175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAx8AAAI/CAYAAAAWf2QnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# drawing graphs\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "models = [RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth=10)] \n",
    "          #DecisionTreeClassifier(max_depth=None), \n",
    "          # BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100, n_jobs=-1),\n",
    "          #SVC(probability=True), \n",
    "          #LogisticRegression(C=81.113083078968728, penalty = 'l1'), \n",
    "          #SGDClassifier(loss='log', random_state=42, alpha=0.01)]\n",
    "model_names = [\"Random Forest\"]# \"Decision Tree\",'SVC','Logistic','SGD-Log']\n",
    "\n",
    "roc_plotting_stuff = []\n",
    "for clf, name in zip(models, model_names):\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    preds = clf.predict_proba(X_test_scaled)\n",
    "    fpr, tpr, _ = roc_curve(y_test, preds[:,1])\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    roc_plotting_stuff.append((name, tpr, fpr, auc_score))\n",
    "    \n",
    "    \n",
    "plt.figure(dpi=150)\n",
    "for name, tpr, fpr, auc_score in roc_plotting_stuff:\n",
    "    plt.plot(fpr, tpr, label=name+' (auc: %.2f)'%auc_score)\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.plot([0, 1], [0, 1], color='k', linestyle='--');\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Comparing ROC Curve\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T02:32:59.741298Z",
     "start_time": "2018-08-07T02:27:10.249917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN CV_recall is: 0.943967901626\n",
      "Logit CV_recall is: 0.842397776721\n",
      "Tree CV_recall is: 0.875558462379\n",
      "Forest CV_recall is: 0.882262653489\n",
      "SVM CV_recall is: 0.947115638708\n",
      "LinearSVM CV_recall is: 0.844068319152\n",
      "GaussianNB CV_recall is: 0.852460229375\n",
      "XGboost CV_recall is: 0.894442242306\n"
     ]
    }
   ],
   "source": [
    "names = ['KNN',\n",
    "        'Logit',\n",
    "        'Tree',\n",
    "        \"Forest\",\n",
    "        'SVM',\n",
    "        'LinearSVM',\n",
    "        'GaussianNB',\n",
    "        'XGboost']\n",
    "\n",
    "clfs = [(KNN(n_neighbors=4)), \n",
    "        (LogisticRegression(C=1000)),\n",
    "        (DecisionTreeClassifier()),\n",
    "        (RandomForestClassifier()),\n",
    "        (SVC()),\n",
    "        (LinearSVC()),\n",
    "        (naive_bayes.GaussianNB()),\n",
    "        (XGBClassifier())\n",
    "         ]\n",
    "for name, clf in zip(names, clfs):\n",
    "    CVScores = cross_val_score(clf, X_train_scaled, y_train, cv=10, scoring='recall')\n",
    "    print(f'{name} CV_recall is:', np.mean(CVScores))\n",
    "    \n",
    "# KNN > SVM > RandomTree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T22:19:05.346182Z",
     "start_time": "2018-08-06T22:19:05.061271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.        ,   8.        ,   8.        , ...,   2.        ,\n",
       "          7.        ,   4.        ],\n",
       "       [  5.43913491,   3.84129764,   7.        , ...,   8.12173018,\n",
       "          7.        ,   7.43913491],\n",
       "       [  5.        ,   6.        ,   6.        , ...,   7.        ,\n",
       "          7.        ,   5.        ],\n",
       "       ..., \n",
       "       [  8.        ,  10.        ,   8.        , ...,   5.        ,\n",
       "          5.        ,   4.        ],\n",
       "       [  7.        ,   5.77618239,   8.        , ...,   3.50717927,\n",
       "          8.22381761,   6.89527046],\n",
       "       [  4.        ,   3.        ,   4.        , ...,   5.        ,\n",
       "          6.        ,   4.        ]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# X_new = SelectKBest(chi2, k=10).fit_transform(X_train, y_train)\n",
    "\n",
    "# X_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T21:11:05.094994Z",
     "start_time": "2018-08-06T20:50:58.313541Z"
    },
    "code_folding": [
     0,
     7,
     10,
     14,
     26
    ]
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-4df7277a5908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{name}: best score: {grid.best_score_}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             train_scores = _score(estimator, X_train, y_train, scorer,\n\u001b[0;32m--> 492\u001b[0;31m                                   is_multimetric)\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \"\"\"\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[0;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 delayed(self._tree.query, check_pickle=False)(\n\u001b[1;32m    384\u001b[0m                     X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             )\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = [('knn', KNN), \n",
    "          ('logistic', LogisticRegression),\n",
    "          ('decision tree', DecisionTreeClassifier),\n",
    "          ('random forest', RandomForestClassifier)\n",
    "         ]\n",
    "\n",
    "param_choices = [\n",
    "    {\n",
    "        'n_neighbors': range(1, 12)\n",
    "    },\n",
    "    {\n",
    "        'C': np.logspace(-3,6, 12),\n",
    "        'penalty': ['l1', 'l2']\n",
    "    },\n",
    "    {\n",
    "        'max_depth': [1,2,3,4,5],\n",
    "        'min_samples_leaf': [3,6,10]\n",
    "    },\n",
    "    {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [1,2,3,4,5],\n",
    "        'min_samples_leaf': [3,6,10]\n",
    "    }\n",
    "]\n",
    "\n",
    "grids = {}\n",
    "for model_info, params in zip(models, param_choices):\n",
    "    name, model = model_info\n",
    "    grid = GridSearchCV(model(), params)\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    y_predict = grid.predict(X_test_scaled)\n",
    "    s = f\"\\n{name}: best score: {grid.best_score_}\"\n",
    "    print(s)\n",
    "    print('recall score: ', '\\t', recall_score(y_test, y_predict))\n",
    "    # I also want to get the recall score because that is the metric that i care about\n",
    "    grids[name] = grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T20:26:29.855355Z",
     "start_time": "2018-08-07T20:22:37.122440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=3, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is going to be for random forest???????? imig\n",
    "\n",
    "# #cool now that i know which model is performing the best, let me know try to find the best paramters of it below\n",
    "params = {\n",
    "        'n_estimators': [ 250, 300],\n",
    "        'max_depth': [5, 7, 10],\n",
    "        'min_samples_leaf': [3, 6, 10]\n",
    "    }\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid = params)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "# y_predict = grid.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# feature_viewer = {}\n",
    "# for col, score in zip(X.columns, grid.feature_importances_):\n",
    "#     feature_viewer[col] = score\n",
    "\n",
    "# print(\"Accuracy: %.3f\"% accuracy_score(y_test, grid.predict(X_test_scaled)))\n",
    "# print(\"Recall: %.3f\"% recall_score(y_test, grid.predict(X_test_scaled)))\n",
    "# print(classification_report(y_test, grid.predict(X_test_scaled)))\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T20:31:07.065522Z",
     "start_time": "2018-08-07T20:31:06.092149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAHXCAYAAAAMbnZJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grid.fit(X_train_scaled, y_train)\n",
    "y_predict = grid.predict(X_test_scaled)\n",
    "\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=y_predict)\n",
    "cm = print_confusion_matrix(conf_mat, ['Class 0', 'Class 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T20:26:56.559022Z",
     "start_time": "2018-08-07T20:26:55.561061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.91      0.91      2071\n",
      "          1       0.91      0.92      0.91      2079\n",
      "\n",
      "avg / total       0.91      0.91      0.91      4150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, grid.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T20:26:58.642262Z",
     "start_time": "2018-08-07T20:26:58.139432Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1, 4150), indices imply (4150, 4150)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   4293\u001b[0m                 blocks = [make_block(values=blocks[0],\n\u001b[0;32m-> 4294\u001b[0;31m                                      placement=slice(0, len(axes[0])))]\n\u001b[0m\u001b[1;32m   4295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   2718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2719\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfastpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim, fastpath)\u001b[0m\n\u001b[1;32m    114\u001b[0m                              'implies %d' % (len(self.values),\n\u001b[0;32m--> 115\u001b[0;31m                                              len(self.mgr_locs)))\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 1, placement implies 4150",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-497-5e29c4379d90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Class 0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Class 1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5d9841616cdc>\u001b[0m in \u001b[0;36mprint_confusion_matrix\u001b[0;34m(confusion_matrix, class_names, figsize, fontsize)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \"\"\"\n\u001b[1;32m     22\u001b[0m     df_cm = pd.DataFrame(\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 306\u001b[0;31m                                          copy=copy)\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_ndarray\u001b[0;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   4301\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4302\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4303\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   4278\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4279\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 4280\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   4281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1, 4150), indices imply (4150, 4150)"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(y_test, grid.predict(X_test_scaled))['Class 0', 'Class 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T19:21:58.715310Z",
     "start_time": "2018-08-07T19:21:58.700786Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-481-e17198adea49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "params = grid.best_params_\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T13:13:20.132365Z",
     "start_time": "2018-08-08T13:13:19.853466Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_estimators must be an integer, got <class 'list'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-514-af36446190fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# Check parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbootstrap\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moob_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/base.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\"n_estimators must be an integer, \"\n\u001b[0;32m--> 105\u001b[0;31m                              \"got {0}.\".format(type(self.n_estimators)))\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_estimators must be an integer, got <class 'list'>."
     ]
    }
   ],
   "source": [
    "X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('random', RandomForestClassifier(**params))])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train_scaled,y_train)\n",
    "print(classification_report(y_test, pipeline.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T13:13:34.558053Z",
     "start_time": "2018-08-08T13:13:34.408507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-1.559050</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>-0.696661</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-0.671019</td>\n",
       "      <td>-0.476727</td>\n",
       "      <td>0.181643</td>\n",
       "      <td>0.670465</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.763313</td>\n",
       "      <td>0.608276</td>\n",
       "      <td>2.245364</td>\n",
       "      <td>-0.065951</td>\n",
       "      <td>-0.476232</td>\n",
       "      <td>0.400707</td>\n",
       "      <td>1.290178</td>\n",
       "      <td>-1.047115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-1.167772</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-1.636658</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>0.227987</td>\n",
       "      <td>-1.135955</td>\n",
       "      <td>0.764391</td>\n",
       "      <td>-2.253552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.653293</td>\n",
       "      <td>1.742819</td>\n",
       "      <td>-0.118896</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>0.530404</td>\n",
       "      <td>0.646172</td>\n",
       "      <td>4.698559</td>\n",
       "      <td>-2.004434</td>\n",
       "      <td>-2.459256</td>\n",
       "      <td>-2.208800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>1.017939</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.972133</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>-0.696661</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-0.916966</td>\n",
       "      <td>0.121034</td>\n",
       "      <td>0.393848</td>\n",
       "      <td>0.092069</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.876974</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>0.400854</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-0.343021</td>\n",
       "      <td>0.670828</td>\n",
       "      <td>0.026544</td>\n",
       "      <td>0.274807</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>1.571178</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>1.183332</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>0.227987</td>\n",
       "      <td>0.400707</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>0.415971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>4.010865</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.016147</td>\n",
       "      <td>0.622089</td>\n",
       "      <td>-0.267848</td>\n",
       "      <td>1.074099</td>\n",
       "      <td>-0.478912</td>\n",
       "      <td>0.360111</td>\n",
       "      <td>-1.125639</td>\n",
       "      <td>-0.145912</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.972133</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>1.183332</td>\n",
       "      <td>2.870277</td>\n",
       "      <td>-0.191717</td>\n",
       "      <td>0.400707</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>0.415971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-1.559050</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>0.556668</td>\n",
       "      <td>0.245356</td>\n",
       "      <td>-1.870533</td>\n",
       "      <td>1.169038</td>\n",
       "      <td>0.764391</td>\n",
       "      <td>1.305812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.385215</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>0.556668</td>\n",
       "      <td>0.245356</td>\n",
       "      <td>0.647691</td>\n",
       "      <td>0.400707</td>\n",
       "      <td>0.764391</td>\n",
       "      <td>-0.473870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.315317</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>3.067509</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-0.444660</td>\n",
       "      <td>-0.062343</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>-0.827431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-1.167772</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>0.556668</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>1.487099</td>\n",
       "      <td>0.400707</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>0.415971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.201703</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>1.183332</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>2.326508</td>\n",
       "      <td>-2.672617</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>0.415971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>20.195947</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.105962</td>\n",
       "      <td>0.200369</td>\n",
       "      <td>-1.163331</td>\n",
       "      <td>1.157511</td>\n",
       "      <td>1.513852</td>\n",
       "      <td>-1.387677</td>\n",
       "      <td>-1.236044</td>\n",
       "      <td>2.668933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.385215</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-0.069997</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-0.191717</td>\n",
       "      <td>2.705700</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>-1.363711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.397343</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>0.243335</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>1.487099</td>\n",
       "      <td>-2.672617</td>\n",
       "      <td>-1.684129</td>\n",
       "      <td>5.755016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.016283</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>0.243335</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-0.012143</td>\n",
       "      <td>-0.141139</td>\n",
       "      <td>-0.707563</td>\n",
       "      <td>0.066232</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.788621</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-1.323326</td>\n",
       "      <td>0.245356</td>\n",
       "      <td>-1.031125</td>\n",
       "      <td>0.400707</td>\n",
       "      <td>2.396738</td>\n",
       "      <td>-1.363711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.189575</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>-1.009994</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>0.227987</td>\n",
       "      <td>-0.521291</td>\n",
       "      <td>0.764391</td>\n",
       "      <td>1.305812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.972133</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>0.243335</td>\n",
       "      <td>0.245356</td>\n",
       "      <td>0.227987</td>\n",
       "      <td>1.169038</td>\n",
       "      <td>0.764391</td>\n",
       "      <td>-0.473870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.592982</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>0.556668</td>\n",
       "      <td>1.120329</td>\n",
       "      <td>-1.051271</td>\n",
       "      <td>0.325410</td>\n",
       "      <td>-0.131767</td>\n",
       "      <td>-0.539718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>4.968967</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.201703</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>0.556668</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-0.410802</td>\n",
       "      <td>-0.000362</td>\n",
       "      <td>-0.121973</td>\n",
       "      <td>0.339444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>1.179900</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-0.696661</td>\n",
       "      <td>1.120329</td>\n",
       "      <td>-0.736493</td>\n",
       "      <td>0.234747</td>\n",
       "      <td>0.212658</td>\n",
       "      <td>-0.739043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.496127</td>\n",
       "      <td>0.212682</td>\n",
       "      <td>-0.370964</td>\n",
       "      <td>1.137594</td>\n",
       "      <td>-0.191717</td>\n",
       "      <td>1.161458</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>-0.482649</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-1.363411</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-0.696661</td>\n",
       "      <td>0.245356</td>\n",
       "      <td>0.647691</td>\n",
       "      <td>-1.135955</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>0.415971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.788621</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>1.120329</td>\n",
       "      <td>-1.031125</td>\n",
       "      <td>4.549694</td>\n",
       "      <td>-0.704721</td>\n",
       "      <td>-1.007775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.972133</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-1.374289</td>\n",
       "      <td>-0.483086</td>\n",
       "      <td>0.087411</td>\n",
       "      <td>0.911692</td>\n",
       "      <td>-0.867956</td>\n",
       "      <td>-0.324848</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>2.235156</td>\n",
       "      <td>-0.809406</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>2.747053</td>\n",
       "      <td>-1.210197</td>\n",
       "      <td>-0.682028</td>\n",
       "      <td>-0.238367</td>\n",
       "      <td>-0.730650</td>\n",
       "      <td>0.116573</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-1.167772</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-1.636658</td>\n",
       "      <td>1.120329</td>\n",
       "      <td>-1.031125</td>\n",
       "      <td>0.400707</td>\n",
       "      <td>3.212912</td>\n",
       "      <td>-1.363711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9652</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>2.235156</td>\n",
       "      <td>-0.189575</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-1.323326</td>\n",
       "      <td>-1.504592</td>\n",
       "      <td>-0.579524</td>\n",
       "      <td>0.282384</td>\n",
       "      <td>-0.177473</td>\n",
       "      <td>-0.062764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9653</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.592982</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>0.243335</td>\n",
       "      <td>1.120329</td>\n",
       "      <td>2.326508</td>\n",
       "      <td>-2.672617</td>\n",
       "      <td>4.845259</td>\n",
       "      <td>-3.143393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9654</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.054451</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>-0.367830</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-0.212478</td>\n",
       "      <td>-0.329618</td>\n",
       "      <td>0.683646</td>\n",
       "      <td>-0.341821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9655</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.972133</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-0.069997</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-0.084273</td>\n",
       "      <td>-0.057219</td>\n",
       "      <td>0.157158</td>\n",
       "      <td>-0.870739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9656</th>\n",
       "      <td>0.771452</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-1.291272</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>0.127799</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-0.480271</td>\n",
       "      <td>0.202490</td>\n",
       "      <td>-0.061707</td>\n",
       "      <td>0.405150</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9657</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.580854</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-1.323326</td>\n",
       "      <td>-1.504592</td>\n",
       "      <td>0.227987</td>\n",
       "      <td>-1.904286</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>1.305812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9658</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.916991</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>0.371574</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>3.177255</td>\n",
       "      <td>-2.120844</td>\n",
       "      <td>0.178260</td>\n",
       "      <td>-1.756748</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9659</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>1.808509</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-0.886714</td>\n",
       "      <td>0.776073</td>\n",
       "      <td>-0.065112</td>\n",
       "      <td>0.027880</td>\n",
       "      <td>-0.150793</td>\n",
       "      <td>-0.608082</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9660</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>1.179900</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-1.323326</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-1.199007</td>\n",
       "      <td>0.400707</td>\n",
       "      <td>0.764391</td>\n",
       "      <td>1.305812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9661</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.397343</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>0.243335</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>1.487099</td>\n",
       "      <td>-2.672617</td>\n",
       "      <td>-1.684129</td>\n",
       "      <td>5.755016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9662</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.972133</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>1.496665</td>\n",
       "      <td>1.120329</td>\n",
       "      <td>-0.527480</td>\n",
       "      <td>-0.213958</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>-0.295902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9663</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>1.203821</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>0.501664</td>\n",
       "      <td>1.120329</td>\n",
       "      <td>-0.643685</td>\n",
       "      <td>-0.140724</td>\n",
       "      <td>-0.598271</td>\n",
       "      <td>0.120103</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9664</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.385215</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-0.383329</td>\n",
       "      <td>2.870277</td>\n",
       "      <td>-0.471240</td>\n",
       "      <td>-0.111002</td>\n",
       "      <td>-0.595354</td>\n",
       "      <td>-0.176663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9665</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.690461</td>\n",
       "      <td>-0.210148</td>\n",
       "      <td>0.871090</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-0.394971</td>\n",
       "      <td>0.028620</td>\n",
       "      <td>-0.347254</td>\n",
       "      <td>0.093830</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>4.010865</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9666</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.385215</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-0.383329</td>\n",
       "      <td>2.870277</td>\n",
       "      <td>-0.471240</td>\n",
       "      <td>-0.111002</td>\n",
       "      <td>-0.595354</td>\n",
       "      <td>-0.176663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9667</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.592982</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>1.183332</td>\n",
       "      <td>2.870277</td>\n",
       "      <td>0.647691</td>\n",
       "      <td>-2.672617</td>\n",
       "      <td>1.580565</td>\n",
       "      <td>2.195652</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>4.010865</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9668</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-1.214554</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-0.087026</td>\n",
       "      <td>0.292909</td>\n",
       "      <td>0.182367</td>\n",
       "      <td>1.127280</td>\n",
       "      <td>0.764391</td>\n",
       "      <td>-0.930397</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9669</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>2.235156</td>\n",
       "      <td>0.006064</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-1.949991</td>\n",
       "      <td>1.120329</td>\n",
       "      <td>4.844732</td>\n",
       "      <td>-1.135955</td>\n",
       "      <td>-1.684129</td>\n",
       "      <td>2.195652</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9670</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>2.249659</td>\n",
       "      <td>-0.474830</td>\n",
       "      <td>0.410022</td>\n",
       "      <td>-1.349436</td>\n",
       "      <td>1.487099</td>\n",
       "      <td>0.128216</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>0.100386</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>4.010865</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9671</th>\n",
       "      <td>0.533371</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>-0.895397</td>\n",
       "      <td>0.787611</td>\n",
       "      <td>-0.696661</td>\n",
       "      <td>-0.420304</td>\n",
       "      <td>-0.191717</td>\n",
       "      <td>0.033101</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>0.415971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9672</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.084033</td>\n",
       "      <td>0.240243</td>\n",
       "      <td>-0.701553</td>\n",
       "      <td>1.537329</td>\n",
       "      <td>-0.824549</td>\n",
       "      <td>-0.494904</td>\n",
       "      <td>2.358516</td>\n",
       "      <td>-0.487761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9673</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.006064</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>1.496665</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-0.863243</td>\n",
       "      <td>0.400707</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>0.415971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9674</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.592982</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>-0.383329</td>\n",
       "      <td>-1.504592</td>\n",
       "      <td>0.647691</td>\n",
       "      <td>-0.367624</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>0.415971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9675</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>1.571178</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>0.556668</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-0.191717</td>\n",
       "      <td>0.400707</td>\n",
       "      <td>-0.867956</td>\n",
       "      <td>-0.473870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9676</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.006064</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>0.243335</td>\n",
       "      <td>1.120329</td>\n",
       "      <td>-0.275658</td>\n",
       "      <td>0.400707</td>\n",
       "      <td>-0.215017</td>\n",
       "      <td>-0.651838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9677</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.006064</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>0.243335</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-0.191717</td>\n",
       "      <td>0.400707</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>-0.473870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9678</th>\n",
       "      <td>0.248596</td>\n",
       "      <td>1.203945</td>\n",
       "      <td>-0.797674</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>0.438119</td>\n",
       "      <td>1.120329</td>\n",
       "      <td>-0.695913</td>\n",
       "      <td>0.437854</td>\n",
       "      <td>-0.232523</td>\n",
       "      <td>-0.022266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>1.351563</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9679</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>2.235156</td>\n",
       "      <td>-1.559050</td>\n",
       "      <td>-0.858520</td>\n",
       "      <td>2.436662</td>\n",
       "      <td>1.120329</td>\n",
       "      <td>-1.702652</td>\n",
       "      <td>6.547355</td>\n",
       "      <td>-0.867956</td>\n",
       "      <td>-1.719647</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9680</th>\n",
       "      <td>-1.023871</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>0.084853</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>0.682855</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>0.146334</td>\n",
       "      <td>-0.826529</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>-0.300753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9681</th>\n",
       "      <td>1.023040</td>\n",
       "      <td>-0.490409</td>\n",
       "      <td>1.571178</td>\n",
       "      <td>1.305231</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>-0.629618</td>\n",
       "      <td>-1.031125</td>\n",
       "      <td>0.400707</td>\n",
       "      <td>-0.051782</td>\n",
       "      <td>0.415971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036703</td>\n",
       "      <td>-0.266930</td>\n",
       "      <td>-0.211880</td>\n",
       "      <td>-0.164653</td>\n",
       "      <td>-0.051041</td>\n",
       "      <td>-0.092214</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>-0.075484</td>\n",
       "      <td>-0.033668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9682 rows Ã— 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     1.023040 -0.490409 -1.559050  1.305231 -0.696661 -0.629618 -0.671019   \n",
       "1     1.023040 -0.490409 -0.763313  0.608276  2.245364 -0.065951 -0.476232   \n",
       "2    -1.023871 -0.490409 -1.167772 -0.858520 -1.636658 -0.629618  0.227987   \n",
       "3     0.653293  1.742819 -0.118896 -0.858520  0.530404  0.646172  4.698559   \n",
       "4     1.023040 -0.490409 -0.972133  1.305231 -0.696661 -0.629618 -0.916966   \n",
       "5    -1.023871 -0.490409 -0.876974 -0.858520  0.400854 -0.629618 -0.343021   \n",
       "6    -1.023871 -0.490409  1.571178  1.305231  1.183332 -0.629618  0.227987   \n",
       "7     1.023040 -0.490409  0.016147  0.622089 -0.267848  1.074099 -0.478912   \n",
       "8     1.023040 -0.490409 -0.972133 -0.858520  1.183332  2.870277 -0.191717   \n",
       "9     1.023040 -0.490409 -1.559050 -0.858520  0.556668  0.245356 -1.870533   \n",
       "10   -1.023871 -0.490409 -0.385215 -0.858520  0.556668  0.245356  0.647691   \n",
       "11    1.023040 -0.490409 -0.315317  1.305231  3.067509 -0.629618 -0.444660   \n",
       "12   -1.023871 -0.490409 -1.167772 -0.858520  0.556668 -0.629618  1.487099   \n",
       "13   -1.023871 -0.490409  0.201703  1.305231  1.183332 -0.629618  2.326508   \n",
       "14   -1.023871 -0.490409  0.105962  0.200369 -1.163331  1.157511  1.513852   \n",
       "15   -1.023871 -0.490409 -0.385215 -0.858520 -0.069997 -0.629618 -0.191717   \n",
       "16   -1.023871 -0.490409  0.397343  1.305231  0.243335 -0.629618  1.487099   \n",
       "17   -1.023871 -0.490409  0.016283  1.305231  0.243335 -0.629618 -0.012143   \n",
       "18   -1.023871 -0.490409  0.788621 -0.858520 -1.323326  0.245356 -1.031125   \n",
       "19   -1.023871 -0.490409 -0.189575  1.305231 -1.009994 -0.629618  0.227987   \n",
       "20   -1.023871 -0.490409 -0.972133 -0.858520  0.243335  0.245356  0.227987   \n",
       "21    1.023040 -0.490409  0.592982  1.305231  0.556668  1.120329 -1.051271   \n",
       "22   -1.023871 -0.490409  0.201703 -0.858520  0.556668 -0.629618 -0.410802   \n",
       "23   -1.023871 -0.490409  1.179900 -0.858520 -0.696661  1.120329 -0.736493   \n",
       "24    1.023040 -0.490409  0.496127  0.212682 -0.370964  1.137594 -0.191717   \n",
       "25   -1.023871 -0.490409 -1.363411 -0.858520 -0.696661  0.245356  0.647691   \n",
       "26    1.023040 -0.490409  0.788621 -0.858520  0.870000  1.120329 -1.031125   \n",
       "27    1.023040 -0.490409 -0.972133 -0.858520 -1.374289 -0.483086  0.087411   \n",
       "28    1.023040  2.235156 -0.809406 -0.858520  2.747053 -1.210197 -0.682028   \n",
       "29    1.023040 -0.490409 -1.167772 -0.858520 -1.636658  1.120329 -1.031125   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9652  1.023040  2.235156 -0.189575 -0.858520 -1.323326 -1.504592 -0.579524   \n",
       "9653  1.023040 -0.490409  0.592982 -0.858520  0.243335  1.120329  2.326508   \n",
       "9654  1.023040 -0.490409  0.054451  1.305231 -0.367830 -0.629618 -0.212478   \n",
       "9655 -1.023871 -0.490409 -0.972133 -0.858520 -0.069997 -0.629618 -0.084273   \n",
       "9656  0.771452 -0.490409 -1.291272  1.305231  0.127799 -0.629618 -0.480271   \n",
       "9657  1.023040 -0.490409 -0.580854 -0.858520 -1.323326 -1.504592  0.227987   \n",
       "9658 -1.023871 -0.490409 -0.916991  1.305231  0.371574 -0.629618  3.177255   \n",
       "9659  1.023040 -0.490409  1.808509 -0.858520 -0.886714  0.776073 -0.065112   \n",
       "9660  1.023040 -0.490409  1.179900 -0.858520 -1.323326 -0.629618 -1.199007   \n",
       "9661 -1.023871 -0.490409  0.397343  1.305231  0.243335 -0.629618  1.487099   \n",
       "9662 -1.023871 -0.490409 -0.972133 -0.858520  1.496665  1.120329 -0.527480   \n",
       "9663  1.023040 -0.490409  1.203821 -0.858520  0.501664  1.120329 -0.643685   \n",
       "9664 -1.023871 -0.490409 -0.385215 -0.858520 -0.383329  2.870277 -0.471240   \n",
       "9665 -1.023871 -0.490409  0.690461 -0.210148  0.871090 -0.629618 -0.394971   \n",
       "9666 -1.023871 -0.490409 -0.385215 -0.858520 -0.383329  2.870277 -0.471240   \n",
       "9667 -1.023871 -0.490409  0.592982 -0.858520  1.183332  2.870277  0.647691   \n",
       "9668  1.023040 -0.490409 -1.214554 -0.858520 -0.087026  0.292909  0.182367   \n",
       "9669 -1.023871  2.235156  0.006064 -0.858520 -1.949991  1.120329  4.844732   \n",
       "9670 -1.023871 -0.490409  2.249659 -0.474830  0.410022 -1.349436  1.487099   \n",
       "9671  0.533371 -0.490409 -0.895397  0.787611 -0.696661 -0.420304 -0.191717   \n",
       "9672  1.023040 -0.490409  0.084033  0.240243 -0.701553  1.537329 -0.824549   \n",
       "9673  1.023040 -0.490409  0.006064 -0.858520  1.496665 -0.629618 -0.863243   \n",
       "9674 -1.023871 -0.490409  0.592982 -0.858520 -0.383329 -1.504592  0.647691   \n",
       "9675 -1.023871 -0.490409  1.571178  1.305231  0.556668 -0.629618 -0.191717   \n",
       "9676  1.023040 -0.490409  0.006064 -0.858520  0.243335  1.120329 -0.275658   \n",
       "9677 -1.023871 -0.490409  0.006064 -0.858520  0.243335 -0.629618 -0.191717   \n",
       "9678  0.248596  1.203945 -0.797674 -0.858520  0.438119  1.120329 -0.695913   \n",
       "9679  1.023040  2.235156 -1.559050 -0.858520  2.436662  1.120329 -1.702652   \n",
       "9680 -1.023871 -0.490409  0.084853  1.305231  0.682855 -0.629618  0.146334   \n",
       "9681  1.023040 -0.490409  1.571178  1.305231  0.870000 -0.629618 -1.031125   \n",
       "\n",
       "           7         8         9      ...          240       241       242  \\\n",
       "0    -0.476727  0.181643  0.670465    ...    -0.036703 -0.266930 -0.211880   \n",
       "1     0.400707  1.290178 -1.047115    ...    -0.036703 -0.266930 -0.211880   \n",
       "2    -1.135955  0.764391 -2.253552    ...    -0.036703 -0.266930 -0.211880   \n",
       "3    -2.004434 -2.459256 -2.208800    ...    -0.036703 -0.266930 -0.211880   \n",
       "4     0.121034  0.393848  0.092069    ...    -0.036703 -0.266930 -0.211880   \n",
       "5     0.670828  0.026544  0.274807    ...    -0.036703 -0.266930 -0.211880   \n",
       "6     0.400707 -0.051782  0.415971    ...    -0.036703  4.010865 -0.211880   \n",
       "7     0.360111 -1.125639 -0.145912    ...    -0.036703 -0.266930 -0.211880   \n",
       "8     0.400707 -0.051782  0.415971    ...    -0.036703 -0.266930 -0.211880   \n",
       "9     1.169038  0.764391  1.305812    ...    -0.036703 -0.266930 -0.211880   \n",
       "10    0.400707  0.764391 -0.473870    ...    -0.036703 -0.266930 -0.211880   \n",
       "11   -0.062343 -0.051782 -0.827431    ...    -0.036703 -0.266930 -0.211880   \n",
       "12    0.400707 -0.051782  0.415971    ...    -0.036703 -0.266930 -0.211880   \n",
       "13   -2.672617 -0.051782  0.415971    ...    -0.036703 -0.266930 -0.211880   \n",
       "14   -1.387677 -1.236044  2.668933    ...    -0.036703 -0.266930 -0.211880   \n",
       "15    2.705700 -0.051782 -1.363711    ...    -0.036703 -0.266930 -0.211880   \n",
       "16   -2.672617 -1.684129  5.755016    ...    -0.036703 -0.266930 -0.211880   \n",
       "17   -0.141139 -0.707563  0.066232    ...    -0.036703 -0.266930 -0.211880   \n",
       "18    0.400707  2.396738 -1.363711    ...    -0.036703 -0.266930 -0.211880   \n",
       "19   -0.521291  0.764391  1.305812    ...    -0.036703 -0.266930 -0.211880   \n",
       "20    1.169038  0.764391 -0.473870    ...    -0.036703 -0.266930 -0.211880   \n",
       "21    0.325410 -0.131767 -0.539718    ...    -0.036703 -0.266930  4.968967   \n",
       "22   -0.000362 -0.121973  0.339444    ...    -0.036703 -0.266930 -0.211880   \n",
       "23    0.234747  0.212658 -0.739043    ...    -0.036703 -0.266930 -0.211880   \n",
       "24    1.161458 -0.051782 -0.482649    ...    -0.036703 -0.266930 -0.211880   \n",
       "25   -1.135955 -0.051782  0.415971    ...    -0.036703 -0.266930 -0.211880   \n",
       "26    4.549694 -0.704721 -1.007775    ...    -0.036703 -0.266930 -0.211880   \n",
       "27    0.911692 -0.867956 -0.324848    ...    -0.036703 -0.266930 -0.211880   \n",
       "28   -0.238367 -0.730650  0.116573    ...    -0.036703 -0.266930 -0.211880   \n",
       "29    0.400707  3.212912 -1.363711    ...    -0.036703 -0.266930 -0.211880   \n",
       "...        ...       ...       ...    ...          ...       ...       ...   \n",
       "9652  0.282384 -0.177473 -0.062764    ...    -0.036703 -0.266930 -0.211880   \n",
       "9653 -2.672617  4.845259 -3.143393    ...    -0.036703 -0.266930 -0.211880   \n",
       "9654 -0.329618  0.683646 -0.341821    ...    -0.036703 -0.266930 -0.211880   \n",
       "9655 -0.057219  0.157158 -0.870739    ...    -0.036703 -0.266930 -0.211880   \n",
       "9656  0.202490 -0.061707  0.405150    ...    -0.036703 -0.266930 -0.211880   \n",
       "9657 -1.904286 -0.051782  1.305812    ...    -0.036703 -0.266930 -0.211880   \n",
       "9658 -2.120844  0.178260 -1.756748    ...    -0.036703 -0.266930 -0.211880   \n",
       "9659  0.027880 -0.150793 -0.608082    ...    -0.036703 -0.266930 -0.211880   \n",
       "9660  0.400707  0.764391  1.305812    ...    -0.036703 -0.266930 -0.211880   \n",
       "9661 -2.672617 -1.684129  5.755016    ...    -0.036703 -0.266930 -0.211880   \n",
       "9662 -0.213958 -0.051782 -0.295902    ...    -0.036703 -0.266930 -0.211880   \n",
       "9663 -0.140724 -0.598271  0.120103    ...    -0.036703 -0.266930 -0.211880   \n",
       "9664 -0.111002 -0.595354 -0.176663    ...    -0.036703 -0.266930 -0.211880   \n",
       "9665  0.028620 -0.347254  0.093830    ...    -0.036703  4.010865 -0.211880   \n",
       "9666 -0.111002 -0.595354 -0.176663    ...    -0.036703 -0.266930 -0.211880   \n",
       "9667 -2.672617  1.580565  2.195652    ...    -0.036703  4.010865 -0.211880   \n",
       "9668  1.127280  0.764391 -0.930397    ...    -0.036703 -0.266930 -0.211880   \n",
       "9669 -1.135955 -1.684129  2.195652    ...    -0.036703 -0.266930 -0.211880   \n",
       "9670  0.128216 -0.051782  0.100386    ...    -0.036703  4.010865 -0.211880   \n",
       "9671  0.033101 -0.051782  0.415971    ...    -0.036703 -0.266930 -0.211880   \n",
       "9672 -0.494904  2.358516 -0.487761    ...    -0.036703 -0.266930 -0.211880   \n",
       "9673  0.400707 -0.051782  0.415971    ...    -0.036703 -0.266930 -0.211880   \n",
       "9674 -0.367624 -0.051782  0.415971    ...    -0.036703 -0.266930 -0.211880   \n",
       "9675  0.400707 -0.867956 -0.473870    ...    -0.036703 -0.266930 -0.211880   \n",
       "9676  0.400707 -0.215017 -0.651838    ...    -0.036703 -0.266930 -0.211880   \n",
       "9677  0.400707 -0.051782 -0.473870    ...    -0.036703 -0.266930 -0.211880   \n",
       "9678  0.437854 -0.232523 -0.022266    ...    -0.036703  1.351563 -0.211880   \n",
       "9679  6.547355 -0.867956 -1.719647    ...    -0.036703 -0.266930 -0.211880   \n",
       "9680 -0.826529 -0.051782 -0.300753    ...    -0.036703 -0.266930 -0.211880   \n",
       "9681  0.400707 -0.051782  0.415971    ...    -0.036703 -0.266930 -0.211880   \n",
       "\n",
       "           243        244       245       246       247       248       249  \n",
       "0    -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "1    -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "2    -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "3     1.017939  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "4    -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "5    -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "6    -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "7    -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "8    -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9    -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "10   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "11   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "12   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "13   -0.164653  20.195947 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "14   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "15   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "16   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "17   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "18   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "19   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "20   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "21   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "22   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "23   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "24   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "25   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "26   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "27   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "28   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "29   -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "...        ...        ...       ...       ...       ...       ...       ...  \n",
       "9652 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9653 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9654 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9655 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9656 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9657 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9658 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9659 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9660 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9661 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9662 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9663 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9664 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9665 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9666 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9667 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9668 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9669 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9670 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9671 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9672 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9673 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9674 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9675 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9676 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9677 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9678 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9679 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9680 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "9681 -0.164653  -0.051041 -0.092214 -0.042551 -0.094719 -0.075484 -0.033668  \n",
       "\n",
       "[9682 rows x 250 columns]"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T20:21:30.826482Z",
     "start_time": "2018-08-07T20:21:30.797399Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_estimators must be an integer, got <class 'list'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-490-a0f7fa3f3bc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_adasyn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_adasyn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# Check parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbootstrap\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moob_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/base.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\"n_estimators must be an integer, \"\n\u001b[0;32m--> 105\u001b[0;31m                              \"got {0}.\".format(type(self.n_estimators)))\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_estimators must be an integer, got <class 'list'>."
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('random', RandomForestClassifier(**params))])\n",
    "\n",
    "\n",
    "pipeline.fit(X_adasyn,y_adasyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T19:13:55.161712Z",
     "start_time": "2018-08-07T19:13:53.856337Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 250 and input n_features is 12 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-474-ae8498f988a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[1;32m    466\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_estimator_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    355\u001b[0m                                  \"call `fit` before exploiting the model.\")\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    382\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 250 and input n_features is 12 "
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, grid.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T16:58:33.814768Z",
     "start_time": "2018-08-07T16:58:33.712013Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##random_forest\n",
    "\n",
    "import pickle\n",
    "\n",
    "pickle.dump(pipeline, open('rf_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T20:20:43.471698Z",
     "start_time": "2018-08-07T20:20:42.953061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5857, 250)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is with all the features without any engineering or selection\n",
    "\n",
    "X = df.drop(drop, axis =1, errors = 'ignore') # i am dropping career bc its not a numeric column :[\n",
    "y = df.match\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 444)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T20:20:45.079103Z",
     "start_time": "2018-08-07T20:20:44.507485Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['attr', 'sinc', 'intel', 'fun', 'amb', 'shar', 'attr_o', 'sinc_o', 'intel_o', 'fun_o', 'amb_o', 'shar_o']\n",
    "X_subset = X[features]\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "X_adasyn, y_adasyn = ADASYN(random_state=42).fit_sample(X_subset,y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_adasyn, y_adasyn, test_size = 0.3, random_state = 444)\n",
    "\n",
    "\n",
    "ssX = StandardScaler()\n",
    "X_train_scaled = ssX.fit_transform(X_train)\n",
    "X_test_scaled = ssX.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns = X_subset.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns = X_subset.columns)\n",
    "\n",
    "\n",
    "# list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T03:50:27.166534Z",
     "start_time": "2018-08-07T03:50:26.084763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=81.113083078968728, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is for logistic regression?\n",
    "\n",
    "\n",
    "# #cool now that i know which model is performing the best, let me know try to find the best paramters of it below\n",
    "params = {\n",
    "        'C': np.logspace(-3,6, 12),\n",
    "        'penalty': ['l1', 'l2']\n",
    "    }\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid = params)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "# y_predict = grid.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# feature_viewer = {}\n",
    "# for col, score in zip(X.columns, grid.feature_importances_):\n",
    "#     feature_viewer[col] = score\n",
    "\n",
    "# print(\"Accuracy: %.3f\"% accuracy_score(y_test, grid.predict(X_test_scaled)))\n",
    "# print(\"Recall: %.3f\"% recall_score(y_test, grid.predict(X_test_scaled)))\n",
    "# print(classification_report(y_test, grid.predict(X_test_scaled)))\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T03:53:04.569241Z",
     "start_time": "2018-08-07T03:53:04.560287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73511689563750304"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T03:50:35.655961Z",
     "start_time": "2018-08-07T03:50:35.647922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 81.113083078968728, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = grid.best_estimator_\n",
    "params = grid.best_params_\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T04:39:42.501369Z",
     "start_time": "2018-08-07T04:39:42.480176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75545759865659112"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = grid.predict(X_train_scaled)\n",
    "recall_score(y_train, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T04:41:41.471668Z",
     "start_time": "2018-08-07T04:41:41.418930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logistic', LogisticRegression(C=81.11308307896873, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('logistic', LogisticRegression(C = 81.113083078968728, penalty = 'l1'))])\n",
    "\n",
    "\n",
    "pipeline.fit(X_adasyn,y_adasyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T20:20:55.295979Z",
     "start_time": "2018-08-07T20:20:55.216148Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (13830,12) (250,) (13830,12) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-488-7fed73274319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_adasyn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_adasyn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, copy)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (13830,12) (250,) (13830,12) "
     ]
    }
   ],
   "source": [
    "y_predict = pipeline.predict(X_adasyn)\n",
    "recall_score(y_adasyn, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T04:49:40.402068Z",
     "start_time": "2018-08-07T04:49:40.388337Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(pipeline, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T04:41:53.023456Z",
     "start_time": "2018-08-07T04:41:53.013907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attr',\n",
       " 'sinc',\n",
       " 'intel',\n",
       " 'fun',\n",
       " 'amb',\n",
       " 'shar',\n",
       " 'attr_o',\n",
       " 'sinc_o',\n",
       " 'intel_o',\n",
       " 'fun_o',\n",
       " 'amb_o',\n",
       " 'shar_o']"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X_subset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T06:02:52.732270Z",
     "start_time": "2018-08-07T06:02:52.200552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attr</th>\n",
       "      <th>sinc</th>\n",
       "      <th>intel</th>\n",
       "      <th>fun</th>\n",
       "      <th>amb</th>\n",
       "      <th>shar</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>shar_o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attr  sinc  intel  fun   amb  shar  attr_o  sinc_o  intel_o  fun_o  amb_o  \\\n",
       "0   6.0   9.0    7.0  7.0   6.0   5.0     6.0     8.0      8.0    8.0    8.0   \n",
       "1   7.0   8.0    7.0  8.0   5.0   6.0     7.0     8.0     10.0    7.0    7.0   \n",
       "2   5.0   8.0    9.0  8.0   5.0   7.0    10.0    10.0     10.0   10.0   10.0   \n",
       "3   7.0   6.0    8.0  7.0   6.0   8.0     7.0     8.0      9.0    8.0    9.0   \n",
       "4   5.0   6.0    7.0  7.0   6.0   6.0     8.0     7.0      9.0    6.0    9.0   \n",
       "5   4.0   9.0    7.0  4.0   6.0   4.0     7.0     7.0      8.0    8.0    7.0   \n",
       "6   7.0   6.0    7.0  4.0   6.0   7.0     3.0     6.0      7.0    5.0    8.0   \n",
       "7   4.0   9.0    7.0  6.0   5.0   6.0     6.0     7.0      5.0    6.0    8.0   \n",
       "8   7.0   6.0    8.0  9.0   8.0   8.0     7.0     7.0      8.0    8.0    8.0   \n",
       "9   5.0   6.0    6.0  8.0  10.0   8.0     6.0     6.0      6.0    6.0    6.0   \n",
       "\n",
       "   shar_o  \n",
       "0     6.0  \n",
       "1     5.0  \n",
       "2    10.0  \n",
       "3     8.0  \n",
       "4     7.0  \n",
       "5     7.0  \n",
       "6     7.0  \n",
       "7     6.0  \n",
       "8     9.0  \n",
       "9     6.0  "
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subset.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T17:03:53.285236Z",
     "start_time": "2018-08-07T17:03:53.091177Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = {\n",
    "  'attr': 6,  # \n",
    "  'sinc': 9,    # \n",
    "  'intel': 7,    # \n",
    "  'fun': 7,  # \n",
    "  'amb': 6,  # \n",
    "  'shar': 5,\n",
    "    'attr_o': 6,\n",
    "    'sinc_o': 8,    # M or F\n",
    "  'intel_o': 8,    # int\n",
    "  'fun_o': 8,  # int\n",
    "  'amb_o': 8,  # int\n",
    "  'shar_o': 6# float\n",
    "}\n",
    "\n",
    "def make_prediction(features):\n",
    "    X = np.array([features['attr'], (features['sinc']), features['intel'], \n",
    "                  features['fun'], features['amb'], features['shar'], features['attr_o'],\n",
    "                 features['sinc_o'], features['intel_o'],features['fun_o'], features['amb_o'], \n",
    "                 features['shar_o']]).reshape(1,-1)\n",
    "    prob_survived = pipeline.predict_proba(X)[0, 1]\n",
    "    \n",
    "    result = {\n",
    "        'prediction': int(prob_survived > 0.5),\n",
    "        'prob_matched': prob_survived\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T17:04:12.225284Z",
     "start_time": "2018-08-07T17:04:12.032838Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jennwon/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': 0, 'prob_matched': 0.49089609085532837}"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T20:27:38.822520Z",
     "start_time": "2018-08-07T20:27:38.697318Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_estimators must be an integer, got <class 'list'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-498-833b4e167913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconf_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Class 0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Class 1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# Check parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbootstrap\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moob_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/base.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\"n_estimators must be an integer, \"\n\u001b[0;32m--> 105\u001b[0;31m                              \"got {0}.\".format(type(self.n_estimators)))\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_estimators must be an integer, got <class 'list'>."
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train_scaled, y_train)\n",
    "y_predict = pipeline.predict(X_test_scaled)\n",
    "\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=y_predict)\n",
    "cm = print_confusion_matrix(conf_mat, ['Class 0', 'Class 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T17:04:42.827647Z",
     "start_time": "2018-08-07T17:04:42.658767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.50910391,  0.49089609],\n",
       "       [ 0.37205567,  0.62794433],\n",
       "       [ 0.36753043,  0.63246957],\n",
       "       [ 0.23853505,  0.76146495],\n",
       "       [ 0.57272749,  0.42727251],\n",
       "       [ 0.82763675,  0.17236325],\n",
       "       [ 0.79750828,  0.20249172],\n",
       "       [ 0.81164911,  0.18835089],\n",
       "       [ 0.23325729,  0.76674271],\n",
       "       [ 0.64717309,  0.35282691]])"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict_proba(X_subset.iloc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T17:04:44.671416Z",
     "start_time": "2018-08-07T17:04:44.585604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "5    0\n",
       "6    0\n",
       "7    0\n",
       "8    1\n",
       "9    0\n",
       "Name: match, dtype: int64"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T04:47:07.098865Z",
     "start_time": "2018-08-07T04:47:07.089538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83009144039886185"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "probs = pipeline.predict_proba(X_subset)[:,1]\n",
    "roc_auc_score(y, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T04:44:11.723276Z",
     "start_time": "2018-08-07T04:44:11.703737Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-435-277e9c4448d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-430-a17cd2efa83b>\u001b[0m in \u001b[0;36mmake_prediction\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     X = np.array([features['attr'], (features['sinc']), features['intel'], \n\u001b[0m\u001b[1;32m     18\u001b[0m                   \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fun'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shar'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attr_o'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                  \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sinc_o'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intel_o'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fun_o'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amb_o'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "make_prediction([ row for row in X_subset.iloc[:5,:].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T02:47:12.461029Z",
     "start_time": "2018-08-07T02:47:03.934122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.864\n",
      "Recall: 0.882\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.85      0.86      2071\n",
      "          1       0.85      0.88      0.87      2079\n",
      "\n",
      "avg / total       0.86      0.86      0.86      4150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('logistic', LogisticRegression(params))])\n",
    "\n",
    "\n",
    "pipeline.fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=3, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "model.score(X_test_scaled, y_test)\n",
    "feature_viewer = {}\n",
    "for col, score in zip(X.columns, model.feature_importances_):\n",
    "    feature_viewer[col] = score\n",
    "\n",
    "\n",
    "print(\"Accuracy: %.3f\"% accuracy_score(y_test, model.predict(X_test_scaled)))\n",
    "print(\"Recall: %.3f\"% recall_score(y_test, model.predict(X_test_scaled)))\n",
    "print(classification_report(y_test, model.predict(X_test_scaled))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T03:51:44.421795Z",
     "start_time": "2018-08-07T03:51:44.373707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_o_att: 0.520182\n",
      "gender: 0.508589\n",
      "pf_o_fun: 0.440914\n",
      "samerace: 0.438443\n",
      "pf_o_sha: 0.307148\n",
      "race_o: 0.279966\n",
      "pf_o_amb: -0.252101\n",
      "age_o: -0.220626\n",
      "partner: 0.090259\n",
      "condtn: -0.055133\n",
      "pf_o_int: 0.043640\n",
      "pf_o_sin: -0.020832\n",
      "Accuracy: 0.735\n",
      "Recall: 0.765\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.71      0.73      2071\n",
      "          1       0.72      0.76      0.74      2078\n",
      "\n",
      "avg / total       0.74      0.74      0.73      4149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=81.113083078968728, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "model.score(X_test_scaled, y_test)\n",
    "feature_viewer = {}\n",
    "for name, coef in sorted(zip(X.columns, model.coef_[0]), key=lambda x: abs(x[1]), reverse=True)[:15]:\n",
    "  print(f'{name}: {coef:8.6f}')\n",
    "\n",
    "\n",
    "print(\"Accuracy: %.3f\"% accuracy_score(y_test, model.predict(X_test_scaled)))\n",
    "print(\"Recall: %.3f\"% recall_score(y_test, model.predict(X_test_scaled)))\n",
    "print(classification_report(y_test, model.predict(X_test_scaled))) \n",
    "#df = pd.DataFrame.from_dict(feature_viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T03:39:14.603723Z",
     "start_time": "2018-08-07T03:39:14.329526Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-361-da7a0bede2f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m     \u001b[0msolver_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[1;32m    888\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_get_liblinear_solver_type\u001b[0;34m(multi_class, penalty, loss, dual)\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0merror_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"loss='%s' is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0m_solver_dual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_solver_pen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_solver_dual\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             error_string = (\"The combination of penalty='%s' \"\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('logistic', LogisticRegression(params))])\n",
    "\n",
    "\n",
    "pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T02:56:15.357508Z",
     "start_time": "2018-08-07T02:56:15.309518Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(pipeline, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T22:52:02.495564Z",
     "start_time": "2018-08-06T22:52:02.252442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>attr_o</td>\n",
       "      <td>0.084452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fun_o</td>\n",
       "      <td>0.077240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>like_o</td>\n",
       "      <td>0.114835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>attr</td>\n",
       "      <td>0.060548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>fun</td>\n",
       "      <td>0.073062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>like</td>\n",
       "      <td>0.100665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature  importance\n",
       "16  attr_o    0.084452\n",
       "19   fun_o    0.077240\n",
       "22  like_o    0.114835\n",
       "76    attr    0.060548\n",
       "79     fun    0.073062\n",
       "82    like    0.100665"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(feature_viewer, orient = 'index')\n",
    "df = df.reset_index()\n",
    "#df\n",
    "df.rename(columns = {'index' : 'feature', 0: 'importance'}, inplace = True)\n",
    "df_new = df.loc[df.importance > 0.06]\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T03:14:45.943718Z",
     "start_time": "2018-08-07T03:14:45.921882Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-348-590fb0470c0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m     \u001b[0msolver_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[1;32m    888\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_get_liblinear_solver_type\u001b[0;34m(multi_class, penalty, loss, dual)\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0merror_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"loss='%s' is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0m_solver_dual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_solver_pen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_solver_dual\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             error_string = (\"The combination of penalty='%s' \"\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(params)\n",
    "lr.fit(X_train_scaled,y_train)\n",
    "lr.coef_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T02:59:09.527553Z",
     "start_time": "2018-08-07T02:59:09.489994Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-337-00d1c82c361d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T03:13:33.564293Z",
     "start_time": "2018-08-07T03:13:33.548093Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-345-0547bd67c992>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{name}: {coef:8.6f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "for name, coef in sorted(zip(X.columns, lr.coef_[0]), key=lambda x: abs(x[1]), reverse=True)[:15]:\n",
    "  print(f'{name}: {coef:8.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T02:57:42.602483Z",
     "start_time": "2018-08-07T02:57:42.555521Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['wave__3' 'positions__3' 'race__3.0' 'order__3' 'field_3.0' 'goal_3.0'\\n 'date_3.0' 'go_out_3.0' 'career_c_3.0' 'satis_2' 'numdat_2' 'attr7_2'\\n 'sinc7_2' 'intel7_2' 'fun7_2' 'amb7_2' 'shar7_2' 'attr1_2' 'sinc1_2'\\n 'intel1_2' 'fun1_2' 'amb1_2' 'shar1_2' 'attr4_2' 'sinc4_2' 'intel4_2'\\n 'fun4_2' 'amb4_2' 'shar4_2' 'attr2_2' 'sinc2_2' 'intel2_2' 'fun2_2'\\n 'amb2_2' 'shar2_2' 'attr3_2' 'sinc3_2' 'intel3_2' 'fun3_2' 'amb3_2'\\n 'attr5_2' 'sinc5_2' 'intel5_2' 'fun5_2' 'amb5_2'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-59ed480365b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_scaled_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test_scaled_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2002\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2003\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s not in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['wave__3' 'positions__3' 'race__3.0' 'order__3' 'field_3.0' 'goal_3.0'\\n 'date_3.0' 'go_out_3.0' 'career_c_3.0' 'satis_2' 'numdat_2' 'attr7_2'\\n 'sinc7_2' 'intel7_2' 'fun7_2' 'amb7_2' 'shar7_2' 'attr1_2' 'sinc1_2'\\n 'intel1_2' 'fun1_2' 'amb1_2' 'shar1_2' 'attr4_2' 'sinc4_2' 'intel4_2'\\n 'fun4_2' 'amb4_2' 'shar4_2' 'attr2_2' 'sinc2_2' 'intel2_2' 'fun2_2'\\n 'amb2_2' 'shar2_2' 'attr3_2' 'sinc3_2' 'intel3_2' 'fun3_2' 'amb3_2'\\n 'attr5_2' 'sinc5_2' 'intel5_2' 'fun5_2' 'amb5_2'] not in index\""
     ]
    }
   ],
   "source": [
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns = X.columns)\n",
    "X_train_scaled_subset = X_train_scaled[features]\n",
    "\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns = X.columns)\n",
    "X_test_scaled_subset = X_test_scaled[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T22:41:20.842327Z",
     "start_time": "2018-08-06T22:41:19.977710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.866\n",
      "Recall: 0.888\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.84      0.86      2079\n",
      "          1       0.85      0.89      0.87      2067\n",
      "\n",
      "avg / total       0.87      0.87      0.87      4146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = list(df_new.feature)\n",
    "#X_train_scaled = X_train_scaled[[features]]\n",
    "\n",
    "\n",
    "model_1 = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=3, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "model_1.fit(X_train_scaled_subset, y_train)\n",
    "model_1.score(X_test_scaled_subset, y_test)\n",
    "feature_viewer = {}\n",
    "for col, score in zip(X.columns, model.feature_importances_):\n",
    "    feature_viewer[col] = score\n",
    "\n",
    "\n",
    "print(\"Accuracy: %.3f\"% accuracy_score(y_test, model_1.predict(X_test_scaled_subset)))\n",
    "print(\"Recall: %.3f\"% recall_score(y_test, model_1.predict(X_test_scaled_subset)))\n",
    "print(classification_report(y_test, model_1.predict(X_test_scaled_subset))) \n",
    "#df = pd.DataFrame.from_dict(feature_viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=3, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('RandomForest', RandomForest(**grids['logistic'].best_params_))])\n",
    "\n",
    "# Do the impuation step\n",
    "#X.Age.fillna(X.groupby(['Sex', 'Pclass']).Age.transform(np.median), inplace=True)\n",
    "\n",
    "# pipeline.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example of making a prediction\n",
    "\n",
    "# example = {\n",
    "#   'Pclass': 3,  # int\n",
    "#   'Sex': 'M',    # M or F\n",
    "#   'Age': 22,    # int\n",
    "#   'SibSp': 1,  # int\n",
    "#   'Parch': 0,  # int\n",
    "#   'Fare': 7.25    # float\n",
    "# }\n",
    "\n",
    "# def make_prediction(features):\n",
    "#     X = np.array([features['Pclass'], int(features['Sex'] == 'M'), features['Age'], \n",
    "#                   features['SibSp'], features['Parch'], features['Fare']]).reshape(1,-1)\n",
    "#     prob_survived = pipeline.predict_proba(X)[0, 1]\n",
    "    \n",
    "#     result = {\n",
    "#         'prediction': int(prob_survived > 0.5),\n",
    "#         'prob_survived': prob_survived\n",
    "#     }\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T14:18:28.096471Z",
     "start_time": "2018-08-06T14:18:28.076963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.683\n",
      "Recall: 0.770\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.59      0.65      2061\n",
      "          1       0.66      0.77      0.71      2096\n",
      "\n",
      "avg / total       0.69      0.68      0.68      4157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model = naive_bayes.GaussianNB()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy: %.3f\"% accuracy_score(y_test, model.predict(X_test_scaled)))\n",
    "print(\"Recall: %.3f\"% recall_score(y_test, model.predict(X_test_scaled)))\n",
    "print(classification_report(y_test, model.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T22:43:15.773421Z",
     "start_time": "2018-08-06T22:43:15.763160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iid',\n",
       " 'id',\n",
       " 'gender',\n",
       " 'idg',\n",
       " 'condtn',\n",
       " 'partner',\n",
       " 'pid',\n",
       " 'match',\n",
       " 'int_corr',\n",
       " 'samerace',\n",
       " 'age_o',\n",
       " 'race_o',\n",
       " 'pf_o_att',\n",
       " 'pf_o_sin',\n",
       " 'pf_o_int',\n",
       " 'pf_o_fun',\n",
       " 'pf_o_amb',\n",
       " 'pf_o_sha',\n",
       " 'dec_o',\n",
       " 'attr_o',\n",
       " 'sinc_o',\n",
       " 'intel_o',\n",
       " 'fun_o',\n",
       " 'amb_o',\n",
       " 'shar_o',\n",
       " 'like_o',\n",
       " 'prob_o',\n",
       " 'met_o',\n",
       " 'age',\n",
       " 'imprace',\n",
       " 'imprelig',\n",
       " 'income',\n",
       " 'career',\n",
       " 'sports',\n",
       " 'tvsports',\n",
       " 'exercise',\n",
       " 'dining',\n",
       " 'museums',\n",
       " 'art',\n",
       " 'hiking',\n",
       " 'gaming',\n",
       " 'clubbing',\n",
       " 'reading',\n",
       " 'tv',\n",
       " 'theater',\n",
       " 'movies',\n",
       " 'concerts',\n",
       " 'music',\n",
       " 'shopping',\n",
       " 'yoga',\n",
       " 'exphappy',\n",
       " 'expnum',\n",
       " 'attr1_1',\n",
       " 'sinc1_1',\n",
       " 'intel1_1',\n",
       " 'fun1_1',\n",
       " 'amb1_1',\n",
       " 'shar1_1',\n",
       " 'attr4_1',\n",
       " 'sinc4_1',\n",
       " 'intel4_1',\n",
       " 'fun4_1',\n",
       " 'amb4_1',\n",
       " 'shar4_1',\n",
       " 'attr2_1',\n",
       " 'sinc2_1',\n",
       " 'intel2_1',\n",
       " 'fun2_1',\n",
       " 'amb2_1',\n",
       " 'shar2_1',\n",
       " 'attr3_1',\n",
       " 'sinc3_1',\n",
       " 'fun3_1',\n",
       " 'intel3_1',\n",
       " 'amb3_1',\n",
       " 'attr5_1',\n",
       " 'sinc5_1',\n",
       " 'intel5_1',\n",
       " 'fun5_1',\n",
       " 'amb5_1',\n",
       " 'dec',\n",
       " 'attr',\n",
       " 'sinc',\n",
       " 'intel',\n",
       " 'fun',\n",
       " 'amb',\n",
       " 'shar',\n",
       " 'like',\n",
       " 'prob',\n",
       " 'met',\n",
       " 'match_es',\n",
       " 'attr1_s',\n",
       " 'sinc1_s',\n",
       " 'intel1_s',\n",
       " 'fun1_s',\n",
       " 'amb1_s',\n",
       " 'shar1_s',\n",
       " 'attr3_s',\n",
       " 'sinc3_s',\n",
       " 'intel3_s',\n",
       " 'fun3_s',\n",
       " 'amb3_s',\n",
       " 'satis_2',\n",
       " 'length',\n",
       " 'numdat_2',\n",
       " 'attr7_2',\n",
       " 'sinc7_2',\n",
       " 'intel7_2',\n",
       " 'fun7_2',\n",
       " 'amb7_2',\n",
       " 'shar7_2',\n",
       " 'attr1_2',\n",
       " 'sinc1_2',\n",
       " 'intel1_2',\n",
       " 'fun1_2',\n",
       " 'amb1_2',\n",
       " 'shar1_2',\n",
       " 'attr4_2',\n",
       " 'sinc4_2',\n",
       " 'intel4_2',\n",
       " 'fun4_2',\n",
       " 'amb4_2',\n",
       " 'shar4_2',\n",
       " 'attr2_2',\n",
       " 'sinc2_2',\n",
       " 'intel2_2',\n",
       " 'fun2_2',\n",
       " 'amb2_2',\n",
       " 'shar2_2',\n",
       " 'attr3_2',\n",
       " 'sinc3_2',\n",
       " 'intel3_2',\n",
       " 'fun3_2',\n",
       " 'amb3_2',\n",
       " 'attr5_2',\n",
       " 'sinc5_2',\n",
       " 'intel5_2',\n",
       " 'fun5_2',\n",
       " 'amb5_2',\n",
       " 'you_call',\n",
       " 'them_cal',\n",
       " 'date_3',\n",
       " 'numdat_3',\n",
       " 'num_in_3',\n",
       " 'attr1_3',\n",
       " 'sinc1_3',\n",
       " 'intel1_3',\n",
       " 'fun1_3',\n",
       " 'amb1_3',\n",
       " 'shar1_3',\n",
       " 'attr7_3',\n",
       " 'sinc7_3',\n",
       " 'intel7_3',\n",
       " 'fun7_3',\n",
       " 'amb7_3',\n",
       " 'shar7_3',\n",
       " 'attr4_3',\n",
       " 'sinc4_3',\n",
       " 'intel4_3',\n",
       " 'fun4_3',\n",
       " 'amb4_3',\n",
       " 'shar4_3',\n",
       " 'attr2_3',\n",
       " 'sinc2_3',\n",
       " 'intel2_3',\n",
       " 'fun2_3',\n",
       " 'amb2_3',\n",
       " 'shar2_3',\n",
       " 'attr3_3',\n",
       " 'sinc3_3',\n",
       " 'intel3_3',\n",
       " 'fun3_3',\n",
       " 'amb3_3',\n",
       " 'attr5_3',\n",
       " 'sinc5_3',\n",
       " 'intel5_3',\n",
       " 'fun5_3',\n",
       " 'amb5_3',\n",
       " 'wave__1',\n",
       " 'wave__2',\n",
       " 'wave__3',\n",
       " 'wave__4',\n",
       " 'wave__5',\n",
       " 'wave__6',\n",
       " 'wave__7',\n",
       " 'wave__8',\n",
       " 'wave__9',\n",
       " 'wave__10',\n",
       " 'wave__11',\n",
       " 'wave__12',\n",
       " 'wave__13',\n",
       " 'wave__14',\n",
       " 'wave__15',\n",
       " 'wave__16',\n",
       " 'wave__17',\n",
       " 'wave__18',\n",
       " 'wave__19',\n",
       " 'wave__20',\n",
       " 'wave__21',\n",
       " 'positions__1',\n",
       " 'positions__2',\n",
       " 'positions__3',\n",
       " 'positions__4',\n",
       " 'positions__5',\n",
       " 'positions__6',\n",
       " 'positions__7',\n",
       " 'positions__8',\n",
       " 'positions__9',\n",
       " 'positions__10',\n",
       " 'positions__11',\n",
       " 'positions__12',\n",
       " 'positions__13',\n",
       " 'positions__14',\n",
       " 'positions__15',\n",
       " 'positions__16',\n",
       " 'positions__17',\n",
       " 'positions__18',\n",
       " 'positions__19',\n",
       " 'positions__20',\n",
       " 'positions__21',\n",
       " 'positions__22',\n",
       " 'race__1.0',\n",
       " 'race__2.0',\n",
       " 'race__3.0',\n",
       " 'race__4.0',\n",
       " 'race__6.0',\n",
       " 'order__1',\n",
       " 'order__2',\n",
       " 'order__3',\n",
       " 'order__4',\n",
       " 'order__5',\n",
       " 'order__6',\n",
       " 'order__7',\n",
       " 'order__8',\n",
       " 'order__9',\n",
       " 'order__10',\n",
       " 'order__11',\n",
       " 'order__12',\n",
       " 'order__13',\n",
       " 'order__14',\n",
       " 'order__15',\n",
       " 'order__16',\n",
       " 'order__17',\n",
       " 'order__18',\n",
       " 'order__19',\n",
       " 'order__20',\n",
       " 'order__21',\n",
       " 'order__22',\n",
       " 'field_1.0',\n",
       " 'field_2.0',\n",
       " 'field_3.0',\n",
       " 'field_4.0',\n",
       " 'field_5.0',\n",
       " 'field_6.0',\n",
       " 'field_7.0',\n",
       " 'field_8.0',\n",
       " 'field_9.0',\n",
       " 'field_10.0',\n",
       " 'field_11.0',\n",
       " 'field_12.0',\n",
       " 'field_13.0',\n",
       " 'field_14.0',\n",
       " 'field_15.0',\n",
       " 'field_16.0',\n",
       " 'field_17.0',\n",
       " 'field_18.0',\n",
       " 'goal_1.0',\n",
       " 'goal_1.5',\n",
       " 'goal_2.0',\n",
       " 'goal_3.0',\n",
       " 'goal_4.0',\n",
       " 'goal_5.0',\n",
       " 'goal_6.0',\n",
       " 'date_0.0',\n",
       " 'date_1.0',\n",
       " 'date_2.0',\n",
       " 'date_3.0',\n",
       " 'date_4.0',\n",
       " 'date_5.0',\n",
       " 'date_6.0',\n",
       " 'go_out_0.0',\n",
       " 'go_out_1.0',\n",
       " 'go_out_2.0',\n",
       " 'go_out_3.0',\n",
       " 'go_out_4.0',\n",
       " 'go_out_5.0',\n",
       " 'go_out_6.0',\n",
       " 'career_c_1.0',\n",
       " 'career_c_2.0',\n",
       " 'career_c_3.0',\n",
       " 'career_c_4.0',\n",
       " 'career_c_5.0',\n",
       " 'career_c_6.0',\n",
       " 'career_c_7.0',\n",
       " 'career_c_8.0',\n",
       " 'career_c_8.5',\n",
       " 'career_c_9.0',\n",
       " 'career_c_10.0',\n",
       " 'career_c_11.0',\n",
       " 'career_c_12.0',\n",
       " 'career_c_13.0',\n",
       " 'career_c_14.0',\n",
       " 'career_c_15.0',\n",
       " 'career_c_16.0',\n",
       " 'career_c_17.0']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T14:18:09.334425Z",
     "start_time": "2018-08-06T14:18:09.259805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.630\n",
      "Recall: 0.420\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.84      0.69      2061\n",
      "          1       0.73      0.42      0.53      2096\n",
      "\n",
      "avg / total       0.66      0.63      0.61      4157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = naive_bayes.BernoulliNB()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy: %.3f\"% accuracy_score(y_test, model.predict(X_test_scaled)))\n",
    "print(\"Recall: %.3f\"% recall_score(y_test, model.predict(X_test_scaled)))\n",
    "print(classification_report(y_test, model.predict(X_test_scaled)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T14:59:52.946853Z",
     "start_time": "2018-08-06T14:59:52.937557Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X_centered_projected = pca.fit_transform(X_train_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T15:00:52.858339Z",
     "start_time": "2018-08-06T15:00:52.820773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.763748</td>\n",
       "      <td>-1.145030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.164229</td>\n",
       "      <td>0.782947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.007443</td>\n",
       "      <td>-0.373839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.007443</td>\n",
       "      <td>-0.373839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.375386</td>\n",
       "      <td>0.008990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.775867</td>\n",
       "      <td>-0.371073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.395805</td>\n",
       "      <td>0.780181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.375386</td>\n",
       "      <td>0.008990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.390272</td>\n",
       "      <td>-0.756668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.297830</td>\n",
       "      <td>-0.382138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.164229</td>\n",
       "      <td>0.782947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.529406</td>\n",
       "      <td>-0.379372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.778634</td>\n",
       "      <td>0.397352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.149343</td>\n",
       "      <td>-1.530625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.387506</td>\n",
       "      <td>-1.525092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.015742</td>\n",
       "      <td>1.931434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.755449</td>\n",
       "      <td>1.160243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.784167</td>\n",
       "      <td>1.934200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.766514</td>\n",
       "      <td>-1.913454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.755449</td>\n",
       "      <td>1.160243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.166996</td>\n",
       "      <td>1.551371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.395805</td>\n",
       "      <td>0.780181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.398571</td>\n",
       "      <td>1.548605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.164229</td>\n",
       "      <td>0.782947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.007443</td>\n",
       "      <td>-0.373839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.161463</td>\n",
       "      <td>0.014523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.155930</td>\n",
       "      <td>-1.522326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.375386</td>\n",
       "      <td>0.008990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.778634</td>\n",
       "      <td>0.397352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.010209</td>\n",
       "      <td>0.394585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9667</th>\n",
       "      <td>-0.390272</td>\n",
       "      <td>-0.756668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9668</th>\n",
       "      <td>0.372620</td>\n",
       "      <td>0.777414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9669</th>\n",
       "      <td>-0.395805</td>\n",
       "      <td>0.780181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9670</th>\n",
       "      <td>-0.010209</td>\n",
       "      <td>0.394585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9671</th>\n",
       "      <td>-0.778634</td>\n",
       "      <td>0.397352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9672</th>\n",
       "      <td>2.677893</td>\n",
       "      <td>0.769115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9673</th>\n",
       "      <td>-1.161463</td>\n",
       "      <td>0.014523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9674</th>\n",
       "      <td>-1.164229</td>\n",
       "      <td>0.782947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9675</th>\n",
       "      <td>-0.004677</td>\n",
       "      <td>-1.142263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9676</th>\n",
       "      <td>1.906702</td>\n",
       "      <td>1.540306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9677</th>\n",
       "      <td>1.534938</td>\n",
       "      <td>-1.916220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9678</th>\n",
       "      <td>-0.010209</td>\n",
       "      <td>0.394585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9679</th>\n",
       "      <td>0.000856</td>\n",
       "      <td>-2.679112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9680</th>\n",
       "      <td>-0.010209</td>\n",
       "      <td>0.394585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9681</th>\n",
       "      <td>-1.161463</td>\n",
       "      <td>0.014523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9682</th>\n",
       "      <td>-0.778634</td>\n",
       "      <td>0.397352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9683</th>\n",
       "      <td>-0.012976</td>\n",
       "      <td>1.163010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9684</th>\n",
       "      <td>0.375386</td>\n",
       "      <td>0.008990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9685</th>\n",
       "      <td>1.143810</td>\n",
       "      <td>0.006224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9686</th>\n",
       "      <td>-0.007443</td>\n",
       "      <td>-0.373839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9687</th>\n",
       "      <td>-1.158696</td>\n",
       "      <td>-0.753902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9688</th>\n",
       "      <td>0.375386</td>\n",
       "      <td>0.008990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9689</th>\n",
       "      <td>-0.775867</td>\n",
       "      <td>-0.371073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9690</th>\n",
       "      <td>-0.778634</td>\n",
       "      <td>0.397352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9691</th>\n",
       "      <td>0.375386</td>\n",
       "      <td>0.008990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9692</th>\n",
       "      <td>-0.770335</td>\n",
       "      <td>-1.907921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9693</th>\n",
       "      <td>-1.929887</td>\n",
       "      <td>0.017289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9694</th>\n",
       "      <td>-0.004677</td>\n",
       "      <td>-1.142263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695</th>\n",
       "      <td>-1.161463</td>\n",
       "      <td>0.014523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9696</th>\n",
       "      <td>1.141044</td>\n",
       "      <td>0.774648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9697 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1\n",
       "0     0.763748 -1.145030\n",
       "1    -1.164229  0.782947\n",
       "2    -0.007443 -0.373839\n",
       "3    -0.007443 -0.373839\n",
       "4     0.375386  0.008990\n",
       "5    -0.775867 -0.371073\n",
       "6    -0.395805  0.780181\n",
       "7     0.375386  0.008990\n",
       "8    -0.390272 -0.756668\n",
       "9     2.297830 -0.382138\n",
       "10   -1.164229  0.782947\n",
       "11    1.529406 -0.379372\n",
       "12   -0.778634  0.397352\n",
       "13    1.149343 -1.530625\n",
       "14   -0.387506 -1.525092\n",
       "15   -0.015742  1.931434\n",
       "16    0.755449  1.160243\n",
       "17   -0.784167  1.934200\n",
       "18    0.766514 -1.913454\n",
       "19    0.755449  1.160243\n",
       "20   -1.166996  1.551371\n",
       "21   -0.395805  0.780181\n",
       "22   -0.398571  1.548605\n",
       "23   -1.164229  0.782947\n",
       "24   -0.007443 -0.373839\n",
       "25   -1.161463  0.014523\n",
       "26   -1.155930 -1.522326\n",
       "27    0.375386  0.008990\n",
       "28   -0.778634  0.397352\n",
       "29   -0.010209  0.394585\n",
       "...        ...       ...\n",
       "9667 -0.390272 -0.756668\n",
       "9668  0.372620  0.777414\n",
       "9669 -0.395805  0.780181\n",
       "9670 -0.010209  0.394585\n",
       "9671 -0.778634  0.397352\n",
       "9672  2.677893  0.769115\n",
       "9673 -1.161463  0.014523\n",
       "9674 -1.164229  0.782947\n",
       "9675 -0.004677 -1.142263\n",
       "9676  1.906702  1.540306\n",
       "9677  1.534938 -1.916220\n",
       "9678 -0.010209  0.394585\n",
       "9679  0.000856 -2.679112\n",
       "9680 -0.010209  0.394585\n",
       "9681 -1.161463  0.014523\n",
       "9682 -0.778634  0.397352\n",
       "9683 -0.012976  1.163010\n",
       "9684  0.375386  0.008990\n",
       "9685  1.143810  0.006224\n",
       "9686 -0.007443 -0.373839\n",
       "9687 -1.158696 -0.753902\n",
       "9688  0.375386  0.008990\n",
       "9689 -0.775867 -0.371073\n",
       "9690 -0.778634  0.397352\n",
       "9691  0.375386  0.008990\n",
       "9692 -0.770335 -1.907921\n",
       "9693 -1.929887  0.017289\n",
       "9694 -0.004677 -1.142263\n",
       "9695 -1.161463  0.014523\n",
       "9696  1.141044  0.774648\n",
       "\n",
       "[9697 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(X_centered_projected)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T14:54:55.372399Z",
     "start_time": "2018-08-06T14:54:55.364471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.28770262e-18,   8.62349162e-17])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T14:54:57.939726Z",
     "start_time": "2018-08-06T14:54:57.932449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50208373,  0.49791627])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T14:54:59.291245Z",
     "start_time": "2018-08-06T14:54:59.278878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70710678, -0.70710678],\n",
       "       [ 0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
